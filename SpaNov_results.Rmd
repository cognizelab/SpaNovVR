---
title: "Analyses for Graded encoding of spatial novelty scales in the human brain"
author: "Joern Alexander Quent"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


# Preparation
## Libs and parameters
```{r load_libs}
# Libraries
library(ggplot2)
library(assortedRFunctions)
library(cowplot) 
library(stringr)
library(plyr)
library(mgcv)
library(foreach)
library(doParallel)
library(BayesFactor)
library(ciftiTools)
library(viridis)
library(tidybayes)
library(bayesplot)
library(brms)
#library(lmerTest)
library(data.table)
library(knitr)
#library(caTools) 
#library(e1071) 
library(readxl)
library(ggridges)
library(ggforce)
library(marginaleffects)
library(gghalves)
library(effectsize)
library(rstan)
library(latex2exp)

# For SVM
library(caTools) 
library(e1071) 

# Load ciftiTools and set workbench paths
possible_wb_paths <- c("/usr/bin/wb_command", "/home1/Jaquent/Toolboxes/workbench/bin_rh_linux64/")
load_ciftiTools(possible_wb_paths)
```

<details>
 <summary>Some settings that change from computer to computer that I work on. </summary>
```{r computer_specific_settings}
# Use correct locations and other settings based on computer
if(Sys.info()[4] == "DESKTOP-335I26I"){
  # Work laptop (Windows)
  path2imaging_results2 <- "D:/Seafile/imaging_results"
} else if(Sys.info()[4] == 'DESKTOP-91CQCSQ') {
  # Work desktop (Windows)
  path2imaging_results2 <- "D:/imaging_results"
} else if(Sys.info()[4] == 'alex-Zenbook-UX3404VA-UX3404VA') {
  # Work laptop (Linux)
  path2imaging_results2 <- "/media/alex/shared/Seafile/imaging_results"
} else if(Sys.info()[4] == "greengoblin"){
  # Main desktop PC (Linux)
  path2imaging_results2 <- "/media/alex/work/Seafile/imaging_results" 
} else if(Sys.info()[4] == "GREEN-GOBLIN-WI"){
  # Main desktop PC 
  path2imaging_results2 <- "E:/Seafile/imaging_results" 
} else {
  # Personal laptop (Windows)
  path2imaging_results2 <- "D:/OLM/imaging_results"
}

# Seed
set.seed(20240205)
```
</details>

<details>
 <summary>Click here for detailed session information. </summary>
```{r session_info}
sessioninfo::session_info()
```
</details>

<details>
 <summary>Click here for chunk for figure and other reporting parameters. </summary>
```{r figure_and_report_params}
# Significance cut off 
cutOff <- 1.301 # Because of -log(0.05, 10)

# Parameters how to report means
report_type   <- 1
digits1       <- 2
rounding_type <- "signif"

theme_journal <- function(base_size = 7, linewidth = 0.35, theme_name = "classic") {
  # Select a base theme from https://ggplot2.tidyverse.org/reference/ggtheme.html
  if(theme_name == "grey" | theme_name == "gray"){
    base_theme <- theme_grey(base_size = base_size)
    
  } else if(theme_name == "bw"){
    base_theme <- theme_bw(base_size = base_size)
    
  } else if(theme_name == "linedraw"){
    base_theme <- theme_linedraw(base_size = base_size)
    
  } else if(theme_name == "light"){
    base_theme <- theme_light(base_size = base_size)
    
  } else if(theme_name == "dark"){
    base_theme <- theme_dark(base_size = base_size)
    
  } else if(theme_name == "minimal"){
    base_theme <- theme_minimal(base_size = base_size)
    
  } else if(theme_name == "classic"){
    base_theme <- theme_classic(base_size = base_size)
    
  } else {
    stop("Unknown theme. Check https://ggplot2.tidyverse.org/reference/ggtheme.html")
  }
  
  # Base them plus customisation
  base_theme +
    theme(
      text = element_text(color = "black", size = base_size),
      axis.text = element_text(size = base_size * 0.9),  # 90% of base
      axis.line = element_line(linewidth = linewidth),
      axis.ticks = element_line(linewidth = linewidth),
      legend.text = element_text(size = base_size * 0.9), # 90% of base
      legend.key.size = unit(0.5, "lines"),
      plot.title = element_text(hjust = 0.5, size = base_size))
}

# Updating geom's defaults
update_geom_defaults("point", list(size = 0.5, stroke = 0.25))
update_geom_defaults("line", list(linewidth = 0.10))
update_geom_defaults("density", list(linewidth = 0.25))
update_geom_defaults("boxplot", list(linewidth = 0.25, size = 0.2))
update_geom_defaults("smooth", list(linewidth = 0.5))
update_geom_defaults("hline", list(linewidth = 0.25))
update_geom_defaults("vline", list(linewidth = 0.25))
update_geom_defaults("curve", list(linewidth = 0.25))
update_geom_defaults("segment", list(linewidth = 0.25))

# Parameters for saving figures
## Information: https://www.nature.com/ncomms/submit/how-to-submit
### PDF page: 210 x 276 mm
### Column widths in mm
single_column <- 88
double_column <- 180
dpi           <- 1000
figurePath    <- "figures/SpaNov/"

# Colours used for visualisation
boxplot_border_col  <- "black"
boxplot_point_col   <- "darkgrey"
boxplot_line_col    <- "darkgrey"
base_col            <- c("#7091C0", "#4A66AC", "#364875")
spectral_colours    <- c("#5956a5", "#a60a44")
cool_warm_colours   <- c("#3C4DC1", "#B70B28")
novFam_gradient     <- viridis(n = 6, option = "H", direction = -1)

# Information for Yeo 7
## Names etc. for the networks in Yeo 7
Yeo7_fullNames <- c("Frontoparietal", "Default", "Dorsal Attention", "Limbic", 
                    "Ventral Attention", "Somatomotor", "Visual")
Yeo7_abbr      <- c("Cont", "Default", "DorsAttn", "Limbic", "SalVentAttn", "SomMot", "Vis")

## Colours used for Yeo 7
Yeo7_colours <- c("#E79523", "#CD3E4E", "#00760F", "#DCF8A4", "#C43BFA", "#4682B4", "#781286")
```
</details>

### Cifti files
<details>
 <summary>Click here for support CIFTI files etc. </summary>
```{r support_cifti}
# Loading the support CIFTI files
## Place where to find some of the CIFTI files and parcellations
CIFTI_locations <- "data/ignore_fMRI_version1/sourceFiles/"

## CIFTI files
parcellationFile <- "Q1-Q6_RelatedValidation210.CorticalAreas_dil_Final_Final_Areas_Group_Colors_with_Atlas_ROIs2.32k_fs_LR.dlabel.nii"
CAB_NP           <- "CortexSubcortex_ColeAnticevic_NetPartition_wSubcorGSR_netassignments_LR.dlabel.nii"
surfLeft         <- "S1200.L.inflated_MSMAll.32k_fs_LR.surf.gii"
surfRight        <- "S1200.R.inflated_MSMAll.32k_fs_LR.surf.gii"

## Combine CIFTI_locations with file name
parcellationFile <- paste0(CIFTI_locations, parcellationFile)
CAB_NP           <- paste0(CIFTI_locations, CAB_NP)
surfLeft         <- paste0(CIFTI_locations, surfLeft)
surfRight        <- paste0(CIFTI_locations, surfRight)

## Loading CIFTIw via ciftiTools as xiis
### Get MMP parcellation
MMP_xii <- ciftiTools::read_cifti(parcellationFile,
                                  brainstructures = "all", 
                                  surfL_fname = surfLeft, 
                                  surfR_fname = surfRight)

### Load Yeo 7 parcellation
Yeo7_xii <- ciftiTools::read_cifti("other_stuff/Yeo7.dlabel.nii",
                                  surfL_fname = surfLeft, 
                                  surfR_fname = surfRight)

## Load other stuff
### Load the parcel names for MMP
parcel_names <- read.csv("data/ignore_fMRI_version1/extracted_values/Parcellations/MP1.0_210V_parcel_names.csv", header = FALSE)

### Load the extracted MNI coordinates
MNI_coord <- read.csv("other_stuff/cifti_subcortical_MNI152_coordinates.csv")

### Load the hippocampal projection values
load("other_stuff/projected_HC.RData")
```

</details>

## Custom functions
<details>
 <summary>Click here for code for custom functions. </summary>
```{r custom_functions}
create_str_from_avg_comparisons <- function (x, measure_name = "X" , digits = 2, rounding_type = "round"){
  # Convert to numeric values
  x_num <- suppressWarnings(as.numeric(x))
  
  # Round and create string
  if (rounding_type == "signif") {
      return_string <- paste0(signif(x_num[3], digits), " ", measure_name ,  " (95 % CI [", 
          signif(x_num[4], digits), ", ", signif(x_num[5], digits), 
          "])")
  }
  else if (rounding_type == "round") {
      return_string <- paste0(round(x_num[3], digits), " ", measure_name ,  " (95 % CI [", 
          round(x_num[4], digits), ", ", round(x_num[5], digits), 
          "])")
  }
  else {
      stop("Wrong rounding type. Choose signif or round.")
  }
  return(return_string)
}
```

</details>

## Loading and preparing the data
<details>
 <summary>Click here for loading data. </summary>
```{r load_data}
# Load the data
## Specify paths where the data is saved
path2data <- "data/ignore_fMRI_version1/"
EV_folder <- "ignore_eventTable2/"

## Load the look-up table that contains information of R-numbers which are retracted 
lookupTable  <- read.csv(paste0(path2data, "lookUpTable.csv"))

## Load .RData images of the combined data (all subject in one DF)
load(paste0(path2data, "combined_data/demographics.RData"))
load(paste0(path2data, "combined_data/DW_all_data.RData"))
load(paste0(path2data, "combined_data/OLM_7T_all_data.RData"))
load(paste0(path2data, "combined_data/OLM_3T_all_data.RData"))
load(paste0(path2data, "combined_data/question_data.RData"))

# Select the subjects included in this analysis
## Load the subjects that are included in this analysis
subjectFile <- readLines(paste0(path2data, "SpaNov_subject2analyse.txt"))
subjIDs_R   <- str_split(subjectFile, pattern = ",")[[1]] 
subjIDs     <- lookupTable$anonKey[lookupTable$Rnum %in% subjIDs_R]
# Important note: subjIDs_R do not have the same order as subjIDs!!!!!!!!!!!!!

## Subset to data that is being included in the analysis
OLM_7T_position_data <- OLM_7T_position_data[OLM_7T_position_data$subject %in% subjIDs, ]
demographics         <- demographics[demographics$subject %in% subjIDs, ]
DW_position_data     <- DW_position_data[DW_position_data$subject %in% subjIDs, ]
OLM_7T_logEntries    <- OLM_7T_logEntries[OLM_7T_logEntries$ppid %in% subjIDs, ]
OLM_7T_trial_results <- OLM_7T_trial_results[OLM_7T_trial_results$subject %in% subjIDs, ]
question_data        <- question_data[question_data$subject %in% subjIDs, ]

# Create positionData
positionData <- OLM_7T_position_data

# Add ppid to this data frame
subjects_anon     <- unique(positionData$subject)
positionData$ppid <- NA
for(i in 1:length(subjects_anon)){
  positionData$ppid[positionData$subject == subjects_anon[i]] <- lookupTable$Rnum[lookupTable$anonKey == subjects_anon[i]]
}

# Add ppid to OLM_7T_trial_results
OLM_7T_trial_results$ppid <- NA
for(i in 1:nrow(OLM_7T_trial_results)){
  OLM_7T_trial_results$ppid[i] <- lookupTable$Rnum[lookupTable$anonKey == OLM_7T_trial_results$subject[i]]
}

# Subset to retrieval only
OLM_7T_retrieval <- OLM_7T_trial_results[OLM_7T_trial_results$trialType == "retrieval", ]

# Get the object locations to verify object placement in screenshots
obj_locations <- ddply(OLM_7T_trial_results, c("targets", "objectName", "targetNames"),
                       summarise, object_x_sd = sd(object_x), object_x = mean(object_x),
                       object_z_sd = sd(object_z), object_z = mean(object_z))

# Subset to encoding only
OLM_7T_encoding <- OLM_7T_trial_results[OLM_7T_trial_results$trialType == "encoding", ]

# Get cue times
cues_agg <- data.frame(ppid = OLM_7T_encoding$ppid,
                       trial = OLM_7T_encoding$trial_num,
                       start = OLM_7T_encoding$start_time,
                       end = OLM_7T_encoding$start_time + OLM_7T_encoding$cue,
                       duration = OLM_7T_encoding$cue,
                       block_num = OLM_7T_encoding$block_num)
# Get delay times
delays_agg <- data.frame(ppid = OLM_7T_encoding$ppid,
                         trial = OLM_7T_encoding$trial_num,
                         start = OLM_7T_encoding$start_time + OLM_7T_encoding$cue,
                         end = OLM_7T_encoding$start_time + OLM_7T_encoding$cue + OLM_7T_encoding$delay,
                         duration = OLM_7T_encoding$delay,
                         block_num = OLM_7T_encoding$block_num)
```


```{r down_sample, eval = FALSE}
# Parameter
downsample_value  <- 0.2 #200 msec/0.2 sec

# Add run information
run_info <- ddply(OLM_7T_trial_results, c("block_num", "trial_num"), summarise, N = length(ppid))
positionData$run <- NA

# Loop through all trials
for(i in 1:nrow(run_info)){
  positionData$run[positionData$trial == run_info$trial_num[i]] <- run_info$block_num[i]
}

# Prepare the cluster for a parallel loop
# Create the cluster following https://www.blasbenito.com/post/02_parallelizing_loops_with_r/
my.cluster <- parallel::makeCluster(detectCores() - 2, type = "PSOCK")

# Register it to be used by %dopar%
doParallel::registerDoParallel(cl = my.cluster)

# Algorithm to down sample to a sample every x msec
# For that loop through tempData_currentSubj and check if 200 msec have passed since current time
# Each row where a new run begins is always included. After that only a sample that is 200 msec
# passed the current time. Especially the first sample of a run is important to accurately
# determine when the first image is recorded and that all the onsets correspond to that. 

# Extract unique participants but in order in which they appear. This was verified
# E.g. via positionData$ppid[!duplicated(positionData$ppid)]
subjects <- unique(positionData$ppid)

# Run parallel loop
include <- foreach(i = 1:length(subjIDs_R), .combine = 'c') %dopar% {
  # Subset to current subject
  tempData_currentSubj <- positionData[positionData$ppid == subjIDs_R[i], ]
  
  # Create include variable
  tempInclude <- rep(FALSE, nrow(tempData_currentSubj))
  
  # Loop through the each row
  for(i in 1:nrow(tempData_currentSubj)){
    # First time
    if(i == 1){
      currentTime <- tempData_currentSubj$time[i]
      tempInclude[i]  <- TRUE
    } else {
      # Check if the last row was from a different run
      if(tempData_currentSubj$run[i - 1] != tempData_currentSubj$run[i]){
        # Set new time in this case
        currentTime <- tempData_currentSubj$time[i]
        tempInclude[i]  <- TRUE
      } else {
        # Check ih ith time is larger than currentTime + downsample_value
        if(tempData_currentSubj$time[i] > currentTime + downsample_value){
          # Set new time in this case
          currentTime <- tempData_currentSubj$time[i]
          tempInclude[i] <- TRUE
        }
      }
    }
  }
  # Return
  tempInclude
}

# Stop cluster again
parallel::stopCluster(cl = my.cluster)

# Apply downsampling to positionData
positionData2 <- positionData[include,]


# Check if there is no problem
time_diff <- ddply(positionData2, c("ppid", "run", "trial"), summarise, 
                   mean_time_diff = mean(diff(time)),
                   sd_time_diff   = sd(diff(time)))
# Now I manually checked the run start time of the first two participants

# Get the number of seeds of this analysis
numSeeds  <- 10
limValues <- c(-90,90)

# Bin the environment
x           <- positionData2$pos_x
z           <- positionData2$pos_z
env_sectors <- voronoi_tessellation_grid_binning_2d(x, z,limValues, numSeeds, "hexagon", 
                                                useParallelisation  = TRUE)

# Add result back to the df (sector2 is only for plotting)
positionData2$sector  <- env_sectors
positionData2$sector2 <- factor(env_sectors, levels = sample(unique(env_sectors)))

# Save in intermediate data
save(positionData2, file = "intermediate_data/positionData2.RData")
```

```{r subset_positionData2}
# Load data
load("intermediate_data/positionData2.RData")

# Exclude control trials
no_control <- positionData2[positionData2$trialType != "control", ]

# Include only Run 1, 2 and 3 because the last retrieval run 
# doesn't count as far as this analysis is concerned
no_control <- no_control[no_control$run %in% 1:3, ]
```

</details>

# Results
## Modelling of spatial novelty during naturalistic navigation
### Reporting demographic information 
```{r demographic_information}
n       <- nrow(demographics)
str1    <- mean_SD_str2(demographics$age, type = 1, digits = digits1, rounding_type = rounding_type, measure = "years")
females <- table(demographics$gender)[1]
males   <- table(demographics$gender)[2]
```

- n = `r n`
- Age = `r str1`
- Gender ratio = `r females`/`r males`
- Age range `r range(demographics$age)`

### Figure 1
#### 3D histogramms
```{r function_to_get_seeds}
get_seeds_from_voronoi_tessellation_hexagon <- function(limValues, numSeeds){
  # Create bins like it is done in voronoi_tessellation_grid_binning_2d
  # This is code directly from that function to get the coordinates of each sector
  xLim    <- sort(limValues)
  yLim    <- sort(limValues)
  byValue <- ((xLim[2] - xLim[1])/(numSeeds - 1))
  xRange  <- seq(from = xLim[1], to = xLim[2], by = byValue)
  xStepSize <- xRange[2] - xRange[1]
  x_shiftValue <- xStepSize/4
  yRange <- seq(from = yLim[1], to = yLim[2], by = xStepSize * (sqrt(3)/2))
  yRange <- yRange[1:length(xRange)]
  yRange <- yRange + (yLim[2] - max(yRange))/2
  seeds <- data.frame(x = xRange[1], y = yRange)
  shiftIndex <- rep(c(1, -1), length.out = nrow(seeds))
  seeds$x <- seeds$x + shiftIndex * x_shiftValue
  for(i in 2:length(xRange)){
    seeds <- rbind(seeds, data.frame(x = xRange[i] + shiftIndex * x_shiftValue, y = yRange))
  }
  seeds$sector <- row.names(seeds)
  return(seeds)
}
```

This creates the data necessary to create the 3D histogram using blender.

```{r data_for_blender_3Dhists, eval = FALSE}
# Load data from 
load("E:/research_projects/OLM_project/analysis/ignore_eventTable3/images/SpaNov_event_file_contPM.RData")

# Convert list to data frame
data <- rbindlist(subj_list, idcol = "subject")

# Subject to character
data$subject <- as.character(data$subject)

# Subset to run 1, 2, 3 to include the first retrieval run
data_sub2 <- data[data$run != 4, ]

# Calculate average/sum per subject
sector_average <- ddply(data_sub2, c("subject","sector"), summarise, 
                        visits = sum(visits, na.rm = TRUE),
                        lastVisit = mean(lastVisit, na.rm = TRUE))

# Calculate average per sector
sector_average <- ddply(sector_average, c("sector"), summarise, 
                        visits = mean(visits, na.rm = TRUE),
                        lastVisit = mean(lastVisit, na.rm = TRUE))

# Bin the environment
limValues   <- c(-90,90)
numSeeds    <- 10 # Number of values per axis

# Create bins like it is done in voronoi_tessellation_grid_binning_2d
seeds <- get_seeds_from_voronoi_tessellation_hexagon(limValues, numSeeds)

# Add x & y coordinates to sector_visits. For this just loop the df
sector_average$x <- NA
sector_average$y <- NA
for(i in 1:nrow(sector_average)){
  # Get current sector
  currentSector <- sector_average$sector[i]
  
  # Get corresponding row index in seeds
  rowID <- which(seeds$sector == currentSector)
  
  # Assign the correct coordinates based on rowID
  sector_average$x[i] <- seeds$x[rowID]
  sector_average$y[i] <- seeds$y[rowID]
}


# Prepare data for 3D plot for the average number of visits per persons to each sector
temp_data <- sector_average[,-3]

# Write .csv file
write.csv(x = temp_data, file = "other_stuff/source_data_files/blenderData_avg_visits.csv", 
          quote = FALSE, row.names = FALSE)

# Prepare data for 3D plot for the average number of visits per persons to each sector
## Remove NA values & remove wrong column
temp_data <- na.omit(sector_average)
temp_data <- temp_data[,-2]

# Write .csv file
write.csv(x = temp_data, file = "other_stuff/source_data_files/blenderData_avg_time_since_last_visit.csv", 
          quote = FALSE, row.names = FALSE)

# Colours
barColours <- viridisLite::viridis(n = 5, option = "D")
```

Range of values is 75 to 111321.

#### Distribution of total path lengths
```{r travelled_paths_density}
# Subset to only data from the encoding part
pathData <- positionData[positionData$trialType == 'encoding', ]

# Calculate the how much the perfect participant would need to travel
## Subset to only encoding trials
boolIndex       <- OLM_7T_trial_results$trialType == "encoding"
OLM_7T_encoding <- OLM_7T_trial_results[boolIndex, ]

# Function to calculate the path lengths 
calculate_path_length <- function(pos_x, pos_z){
  # Calculate difference between points and square them
  diff_X <- diff(pos_x)^2
  diff_z <- diff(pos_z)^2
  
  # Calculate the sum and take the square root
  dists_travelled <- sqrt(diff_X + diff_z)
  
  # Sum up all distances travelled to one path_length
  return(sum(dists_travelled))
}

# Calculate the path traveled for each trial and each subject
# Use pathData because it only includes encoding data
path_lenghts <- ddply(pathData, c("ppid", "trial"), 
                      summarise, 
                      distance = calculate_path_length(pos_x, pos_z))

# Calculate the total path lengths for the whole experiment
total_path_lengths <- ddply(path_lenghts, c("ppid"), 
                            summarise, distance = sum(distance))

# Write .csv file
write.csv(x = total_path_lengths, file = "other_stuff/source_data_files/total_path_lengths.csv", 
          quote = FALSE, row.names = FALSE)

# Create a box plot 
total_path_lengths_plot <- ggplot(total_path_lengths, aes(x = distance)) + 
  geom_density(fill = base_col[1]) + 
  geom_jitter(aes(y = -0.001), height = 0.0005, width = 0, colour = boxplot_point_col) +
  labs(y = "Density", x = "Total path length (vm)", title = "") +
  scale_x_continuous(breaks = c(3000, 3300, 3600)) + 
  coord_cartesian(xlim = c(3000, 3600), expand = FALSE, ylim = c(-0.001*2, 0.0065)) +
  theme_journal() +
  theme(axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),
        axis.line.y = element_blank(),
        plot.margin = unit(c(1, 2.7, 1, 2.7), "mm"))
```

#### Distributions of event durations
```{r event_durations}
# Load the event files
event_files <- list.files(path = "event_tables/OLMe_7T_SpaNov_gradient_6lvl/", recursive = TRUE, full.names = TRUE)

# Remove "per_tra.txt" and "dur_tra.txt" from list
event_files <- event_files[!str_detect(event_files, "tra.txt")]

# Remove cue and delay
event_files <- event_files[!str_detect(event_files, "cue.txt") & !str_detect(event_files, "delay.txt")]

# Create list
event_list <- list()

# Load the event files
for(i in 1:length(event_files)){
  # Load current event file
  temp_event_file        <- read.table(event_files[i], header = FALSE, sep = "\t")
  temp_event_file$V3     <- NULL # Remove the third column
  names(temp_event_file) <- c("onset", "duration")
  
  # Split the file path to get information
  event_file_path_split   <- str_split_1(event_files[i], pattern = "/")
  temp_event_file$subject <- event_file_path_split[4]
  temp_event_file$run     <- event_file_path_split[5]
  
  # To extract the level of the event extract "lvl" plus an integer from string
  temp_event_file$lvl     <- str_extract(event_file_path_split[8], "lvl[0-9]+")
  
  # Add to list
  event_list[[i]] <- temp_event_file
}

# Combine to data frame
event_df <- as.data.frame(rbindlist(event_list))

# Calculate offset
event_df$offset <- event_df$onset + event_df$duration

# Convert lvl label to number
event_df$lvl_num <- as.numeric(str_extract(event_df$lvl, "[0-9]+"))

# Change run label
event_df$run <- ifelse(event_df$run == "run-01", "Run 1", "Run 2")

# Write .csv file
write.csv(x = event_df, file = "other_stuff/source_data_files/event_duration.csv", 
          quote = FALSE, row.names = FALSE)

# Create a box plot 
event_duration_plot <- ggplot(event_df, aes(x = duration)) + 
  geom_density(fill = base_col[1]) + 
  geom_jitter(aes(y = -0.1), height = 0.05, width = 0, colour = boxplot_point_col, alpha = 0.1) +
  labs(y = "Density", x = "Event duration (s)", title = "") +
  scale_x_continuous(breaks = c(0, 16, 32)) + 
  coord_cartesian(xlim = c(0, 32), expand = FALSE, ylim = c(-0.1*2, 0.8)) +
  theme_journal() +
  theme(axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),
        axis.line.y = element_blank(),
        plot.margin = unit(c(1, 2.5, 1, 2.5), "mm"))
```

#### Distributions of Rayleigh vector length
Calculate Rayleigh Test of Uniformity for each sector

```{r Rayleigh_Test_of_Uniformity}
#install.packages("circular")  # If not already installed
library(circular)

# Function to calculate the test and apply it to the data frame
calc_rayleigh_test_FUN <- function(angles){
  # Create circular object
  circ_data <- circular(angles, units = "degrees", template = "none")
 
 # Calculate Rayleigh test which includes R
 rayleigh_test <- rayleigh.test(circ_data)
 
 # Return
 return(c(rayleigh_test$statistic, rayleigh_test$p.value))
}

# Apply test to all sectors
sector_rayleigh <- ddply(no_control, c("sector"), summarise,
                         resultant_length = calc_rayleigh_test_FUN(rot_y)[1],
                         p.value = calc_rayleigh_test_FUN(rot_y)[2])

# Write .csv file
write.csv(x = sector_rayleigh, file = "other_stuff/source_data_files/sector_rayleigh.csv", 
          quote = FALSE, row.names = FALSE)

sector_rayleigh_plot <- ggplot(sector_rayleigh, aes(x = resultant_length)) +
  geom_histogram(colour = base_col[1], fill = base_col[1]) + 
  geom_vline(xintercept = mean(sector_rayleigh$resultant_length), colour = "black", linetype = 2, size = 0.5) +
  scale_x_continuous(breaks = c(0, 0.5, 1)) + 
  scale_y_continuous(breaks = c(0, 5, 10)) + 
  coord_cartesian(xlim = c(-0.02, 1), ylim = c(0, 10), 
                  expand = FALSE) +
  labs(x = "Length of Rayleigh vector", y = "Count") +
  theme_journal() +
  theme(plot.margin = unit(c(1, 2.5, 1, 2.5), "mm"))

# Report mean
str1 <- mean_SD_str2(sector_rayleigh$resultant_length, report_type, digits1, rounding_type)
```

#### Time spend in each locomotion state
Calculate overall time spent on translation, rotation and being stationary:

```{r locomotion1}
# Remove cue & delay periods
locomotion_data         <- pathData
locomotion_data$include <- TRUE
subjs  <- unique(locomotion_data$ppid)
trials <- unique(locomotion_data$trial)
for(i in 1:length(subjs)){
  for(j in 1:length(trials)){
    # Get cue start & delay end time
    cue_start <- cues_agg$start[cues_agg$ppid == subjs[i] & cues_agg$trial == trials[j]]
    delay_end <- delays_agg$end[delays_agg$ppid == subjs[i] & delays_agg$trial == trials[j]]
    
    # Set include to false for those times
    bool_index <- locomotion_data$ppid == subjs[i] & 
                  locomotion_data$trial == trials[j] &
                  locomotion_data$time >= cue_start & 
                  locomotion_data$time <= delay_end
    locomotion_data$include[bool_index] <- FALSE
  }
}

# Remove those times
locomotion_data <- locomotion_data[locomotion_data$include, ]

# The amount we found the values to avoid false positive
rotation_round  <- 2 # round rotation values to this decimal point

# Function to determine which state a time point belongs to
what_state <- function(rot_y, moving2){
  # Get angles 
  angle1 <- rot_y[2:length(rot_y)]
  angle2 <- rot_y[1:(length(rot_y)-1)]
  
  # Calculate the amount was rotated between the time points and then rotate
  rotated <- c(NA, round(angularDifference(angle1, angle2), rotation_round))
  
  # If rotation is zero called it stationary, otherwise rotation
  tra_rot_sta <- ifelse(abs(rotated) == 0 | is.na(rotated), 
                        'stationary', 'rotation') 
  
  # Set time point to translation based the information saved by unity
  tra_rot_sta[moving2] <- 'translation'
  
  # Return
  return(tra_rot_sta)
}

# Calculate the state for each time points for each subject and each trial
locomotion_data <- ddply(locomotion_data, c("ppid", "trial"), 
                  mutate, locomotion = what_state(rot_y, moving))

# Calculate the total percentage for each subject
locomotion_per <- ddply(locomotion_data, c("ppid"), summarise,
                        translation = mean(locomotion == "translation"),
                        rotation = mean(locomotion == "rotation"),
                        stationary = mean(locomotion == "stationary"))

# Convert from wide to long format
locomotion_per_long  <- reshape2::melt(locomotion_per, id.vars = c("ppid"))

# Convert proportions to percentage
locomotion_per_long$value <- locomotion_per_long$value * 100

# Write .csv
write.csv(x = locomotion_per_long, file = "other_stuff/source_data_files/locomotion_per_long.csv", 
          quote = FALSE, row.names = FALSE)

# Plot showing percentage of locomotion
locomotion_states_plot <- ggplot(locomotion_per_long, aes(x = variable, y = value)) +
  geom_line(aes(group = ppid), colour = boxplot_line_col) +
  geom_point(colour = boxplot_point_col) +
  geom_boxplot(colour = boxplot_border_col, outlier.shape = NA, width = 0.4,
               fill = base_col[2]) +
  stat_summary(geom = "point", fun = "mean", col = 'white', shape = 24,
               fill = base_col[2], position = position_dodge(width =  0.75), size = 0.8) +
  theme_journal() + 
  #scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
  scale_y_continuous(breaks = c(0, 40, 80)) + 
  coord_cartesian(xlim = c(0.5, 3.5), 
                  ylim = c(0, 80), expand = FALSE) +
  theme(legend.position = "none", plot.margin = unit(c(1, 1, 1, 1), "mm")) +
  labs(title = "", x = "Locomotion", y = "Percentage")
```

#### Novelty score over time model 
```{r noveltyScore_model_plot}
# Load the models
load("fitted_brms_models/SpaNov_m_noveltyScore.Rdata")
# Background: https://www.andrewheiss.com/blog/2022/11/29/conditional-marginal-marginaleffects/
# Set range
x1_range   <- range(m_noveltyScore_run1$data$s_onset_rel)
x1_points  <- seq(from = x1_range[1], to = x1_range[2], length.out = 3)
x2_range   <- range(m_noveltyScore_run2$data$s_onset_rel) 
x2_points  <- seq(from = x2_range[1], to = x2_range[2], length.out = 3)

# Get subject-level predictions
pred1_df <- predictions(m_noveltyScore_run1, newdata = datagrid(s_onset_rel = x1_points, subject = unique), 
                           by = c("s_onset_rel", "subject"))
pred2_df <- predictions(m_noveltyScore_run2, newdata = datagrid(s_onset_rel = x2_points, subject = unique), 
                           by = c("s_onset_rel", "subject"))
pred1_df <- as.data.frame(pred1_df)
pred2_df <- as.data.frame(pred2_df)

# Calculate the minimum for each subject, which is used for colouring
pred1_df <- ddply(pred1_df, c("subject"), mutate, min = min(estimate))
pred2_df <- ddply(pred2_df, c("subject"), mutate, min = min(estimate))

# Create plots
## Run 1
p1 <- ggplot(data = pred1_df, aes(x = s_onset_rel, y = estimate, group = subject, colour = min)) +
  geom_line() +
  scale_color_viridis_c() + theme_journal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 2, 1, 2), "mm")) +
  labs(title = "Run 1", x = "Time", y = "Novelty score") +
  scale_x_continuous(breaks = x1_range, labels = c("Start", "End")) +
  scale_y_continuous(breaks = c(-2.0, -0.7,  0.6)) +
  coord_cartesian(xlim = x1_range, ylim = c(-2.03, 0.6),
                  expand = FALSE) 

## Run 2
p2 <- ggplot(data = pred2_df, aes(x = s_onset_rel, y = estimate, group = subject, colour = min)) +
  geom_line() +
  scale_color_viridis_c() + theme_journal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 2, 1, 2), "mm")) +
  labs(title = "Run 2", x = "Time", y = "Novelty score") +
  scale_x_continuous(breaks = x2_range, labels = c("Start", "End")) +
  scale_y_continuous(breaks = c(-1.5, -0.5, 0.5)) +
  coord_cartesian(xlim = x2_range, ylim = c(-1.53, 0.5),
                  expand = FALSE) 

# Combine and save
novelty_score_model_plot <- plot_grid(p1, p2, ncol = 2)
```

#### Combine to one figure
```{r Figure1_combined}
# Combine
part1 <- plot_grid(total_path_lengths_plot, event_duration_plot, 
                   sector_rayleigh_plot, locomotion_states_plot, align = "h", nrow = 1)
part2 <- plot_grid(NULL, novelty_score_model_plot)
Figure1_combined <- plot_grid(part1, NULL, part2, align = "v", nrow = 3, rel_heights = c(1, 0.2, 1))

# Save
ggsave(Figure1_combined,
       filename = paste0(figurePath, "Fig_1_lower_part.png"), 
       dpi = 1000,
       width = 170,
       height = 90,
       units = "mm")
```

![](figures/SpaNov/Fig_1_lower_part.png)

### Descriptive statistics of exploration
#### Mean and SD of Raleigh vector length
```{r rayleigh_vector_length_descriptive_stats}
# Report mean
str1 <- mean_SD_str2(sector_rayleigh$resultant_length, report_type, digits1, rounding_type)
```

- Raleigh vector length: `r str1`

#### Calculating the number of visits per participant
```{r num_visitedSectors}
# Calculate the number of visited sectors
num_visitedSectors <- ddply(no_control, c("ppid"), summarise, 
                            number = length_uniq(sector))

# Make report string
val  <- num_visitedSectors$number
str1 <- mean_SD_str2(val, report_type, digits1, rounding_type)
```

Number of visited sector per participant: `r str1`

#### Describe two measures on which the novelty score is based
```{r analyse_novelty_measures}
# Load image generated when creating the event files
load("event_tables/images/SpaNov_event_file_gradients.RData")

# Convert list to data frame
data <- as.data.frame(rbindlist(subj_list, idcol = "subject"))

# Add subjects' R number
data$ppid <- subjIDs_R[data$subject]

# Function to match runStartTime
find_runStartTime <- function(ppid, run){
  # Get corresponding to find the run in the trial data
  anonKey <- lookupTable$anonKey[lookupTable$Rnum == ppid[1]]
  
  # Use the anonKey & run to get runStartTime
  bool_index  <- OLM_7T_trial_results$subject == anonKey & 
                 OLM_7T_trial_results$block_num == run
  runStartTime <- OLM_7T_trial_results$runStartTime[bool_index]
  
  return(runStartTime[1])
}

# Use the function to find the correct run start time
data <- ddply(data, c("subject", "ppid", "run"), mutate, 
              runStartTime = find_runStartTime(ppid, run))

# Make time relative to the start of the run. The real onset times will be 
# slightly different but this will not matter for this. 
data$onset_rel <- data$onset - data$runStartTime

# Add run type
data$runType <- "encoding"
data$runType[data$run == 2 | data$run == 4] <- "retrieval"

# Subset to only encoding
data_sub <- data[data$runType == "encoding", ]

# Change run number to match the description in paper. Run 3 is Encoding Run 2
data_sub$run[data_sub$run == 3] <- 2

# Convert run to factor
data_sub$f_run <- as.factor(data_sub$run)


# Calculate descriptive stats for the measures
data_sub_agg1 <- ddply(data_sub, c("subject"), summarise,
                      max_visits = max(visits),
                      min_lastVisit = min(lastVisit, na.rm = TRUE),
                      max_lastVisit = max(lastVisit, na.rm = TRUE),
                      median_lastVisit = median(lastVisit, na.rm = TRUE),
                      mean_lastVisit = mean(lastVisit, na.rm = TRUE))

# Calculate the average duration of events from lastVisits for that exclude NA values
data_sub_agg2 <- ddply(na.omit(data_sub), c("subject"), summarise,
                       mean_duration = mean(duration),
                       median_duration = median(duration),
                       min_duration = min(duration),
                       max_duration = max(duration))

# Create strings for report
val  <- data_sub_agg1$max_visits
str1 <- mean_SD_str2(val, report_type, digits1, rounding_type)
val  <- data_sub_agg1$min_lastVisit
str2 <- mean_SD_str2(val, report_type, digits1, rounding_type, "s")
val  <- data_sub_agg1$max_lastVisit
str3 <- mean_SD_str2(val, report_type, digits1, rounding_type, "s")
val  <- data_sub_agg2$mean_duration
str4 <- mean_SD_str2(val, report_type, digits1, rounding_type, "s")
val  <- data_sub_agg2$min_duration
str5 <- mean_SD_str2(val, report_type, digits1, rounding_type, "s")
val  <- data_sub_agg2$max_duration
str6 <- mean_SD_str2(val, report_type, digits1, rounding_type, "s")
```

- Maximum visit per sector for the participants was `r str1` by the time of the second run.
- The minimum time between visits varied between participants `r str2`.
- The maximum time between visits varied between participants `r str3`.
- The average duration of the events for the time since varied between participants `r str4` with an average minimum of `r str5` and an average maximum of `r str6`. 

#### Locomotion during exploration
Report the average values for the three locomotion states

```{r locomotion2}
val  <- locomotion_per$translation * 100
str1 <- mean_SD_str2(val, report_type, digits1, rounding_type, "%")
val  <- locomotion_per$rotation  * 100
str2 <- mean_SD_str2(val, report_type, digits1, rounding_type, "%")
val  <- locomotion_per$stationary * 100
str3 <- mean_SD_str2(val, report_type, digits1, rounding_type, "%")
```

- Translation: `r str1`
- Rotation: `r str2`
- Stationary: `r str3`

#### Travelled path length
```{r travalled_path_length_report}
# Create the report string
str1 <- mean_SD_str2(total_path_lengths$distance, 
                     report_type, digits1, rounding_type, "vm")
```

The average total path length was `r str1`

#### Event duration
```{r event_duration_report}
# Make report string
val  <- event_df$duration
str1 <- mean_SD_str2(val, report_type, digits1, rounding_type)
```

Event duration: `r str1`

#### Bayesian hierarchical modelling of novelty score
```{r novety_score_model}
# Get marginal results
mae_respone_noveltyScore_run1 <- avg_comparisons(m_noveltyScore_run1, variables = "s_onset_rel", 
                                                 type = "response", re_formula = NULL)
mae_respone_noveltyScore_run2 <- avg_comparisons(m_noveltyScore_run2, variables = "s_onset_rel", 
                                                 type = "response", re_formula = NULL)

str1 <- create_str_from_avg_comparisons(mae_respone_noveltyScore_run1, "SDs")
str2 <- create_str_from_avg_comparisons(mae_respone_noveltyScore_run2, "SDs")
```

- Average marginal effect in Run 1: `r str1`
- Average marginal effect in Run 2: `r str2`

$$
\begin{align*} 
y & \sim \operatorname{Student}(\nu, \mu, \sigma) \\ 
\mu & = time + (time | subject)  
\end{align*} 
$$

Alternative way to describe: https://rpsychologist.com/r-guide-longitudinal-lme-lmer

## Mapping spatial novelty across the hippocampal long axis
Load the results from the linear contrast (Level 1 > Level 2 > Level 3 > ...). All xiis from this contrast start with GLM1. 

- Positive values: Higher activity for familiar sectors (c1)
- Negative values: Higher activity for novel sectors (c2)

```{r Loading_GLM1}
# Folder where the images are
ciftiFolder <- "/SpaNov/OLMe_7T_SpaNov_gradient_6lvl_cue-delay_smo4_MSMAll/cope7.feat/stats/vwc/"

# Other parameters
minClusterSize   <- 20

# Choose the files
zMap_file        <- paste0(path2imaging_results2, ciftiFolder, "results_lvl2cope1_dat_ztstat_c1.dscalar.nii")
pMap1_file       <- paste0(path2imaging_results2, ciftiFolder, "results_lvl2cope1_dat_ztstat_cfdrp_c1.dscalar.nii")
pMap2_file       <- paste0(path2imaging_results2, ciftiFolder, "results_lvl2cope1_dat_ztstat_cfdrp_c2.dscalar.nii")
clusterMap1_file <- paste0(path2imaging_results2, ciftiFolder, "results_lvl2cope1_dat_ztstat_cfdrp_c1_clusters.dscalar.nii")
clusterMap2_file <- paste0(path2imaging_results2, ciftiFolder, "results_lvl2cope1_dat_ztstat_cfdrp_c2_clusters.dscalar.nii")
betaMap_file     <- paste0(path2imaging_results2, ciftiFolder, "Y1.dtseries.nii")

# Load all maps as xiis
GLM1_zMap_xii        <- read_cifti(zMap_file, brainstructures = "all")
GLM1_pMap1_xii       <- read_cifti(pMap1_file, brainstructures = "all")
GLM1_pMap2_xii       <- read_cifti(pMap2_file, brainstructures = "all")
GLM1_clusterMap1_xii <- read_cifti(clusterMap1_file, brainstructures = "all")
GLM1_clusterMap2_xii <- read_cifti(clusterMap2_file, brainstructures = "all")
GLM1_betaMap_xii     <- read_cifti(betaMap_file, brainstructures = "all")

# Create masks based on significant effects
HC_familiarity_mask   <- GLM1_pMap1_xii$data$subcort > cutOff & str_detect(GLM1_pMap1_xii$meta$subcort$labels, pattern = "Hippocampus")
HC_novelty_mask       <- GLM1_pMap2_xii$data$subcort > cutOff & str_detect(GLM1_pMap1_xii$meta$subcort$labels, pattern = "Hippocampus")
HC_familiarity_mask_L <- GLM1_pMap1_xii$data$subcort > cutOff & str_detect(GLM1_pMap1_xii$meta$subcort$labels, pattern = "Hippocampus-L")
HC_novelty_mask_L     <- GLM1_pMap2_xii$data$subcort > cutOff & str_detect(GLM1_pMap1_xii$meta$subcort$labels, pattern = "Hippocampus-L")
HC_familiarity_mask_R <- GLM1_pMap1_xii$data$subcort > cutOff & str_detect(GLM1_pMap1_xii$meta$subcort$labels, pattern = "Hippocampus-R")
HC_novelty_mask_R     <- GLM1_pMap2_xii$data$subcort > cutOff & str_detect(GLM1_pMap1_xii$meta$subcort$labels, pattern = "Hippocampus-R")
WB_familiarity_mask   <- GLM1_pMap1_xii > cutOff 
WB_novelty_mask       <- GLM1_pMap2_xii > cutOff 

# Extract & average values
HC_familiarity_values <- GLM1_betaMap_xii$data$subcort[HC_familiarity_mask, ]
HC_familiarity_values <- colMeans(HC_familiarity_values)
HC_novelty_values     <- GLM1_betaMap_xii$data$subcort[HC_novelty_mask, ]
HC_novelty_values     <- colMeans(HC_novelty_values)

HC_familiarity_values_L <- GLM1_betaMap_xii$data$subcort[HC_familiarity_mask_L, ]
HC_familiarity_values_L <- colMeans(HC_familiarity_values_L)

HC_familiarity_values_R <- GLM1_betaMap_xii$data$subcort[HC_familiarity_mask_R, ]
HC_familiarity_values_R <- colMeans(HC_familiarity_values_R)
HC_novelty_values_R     <- GLM1_betaMap_xii$data$subcort[HC_novelty_mask_R, ]
HC_novelty_values_R     <- colMeans(HC_novelty_values_R)

WB_novelty_values <- as.matrix(GLM1_betaMap_xii)
WB_novelty_values <- WB_novelty_values[as.matrix(WB_novelty_mask) == 1, ]
WB_novelty_values <- colMeans(WB_novelty_values)

WB_familiarity_values <- as.matrix(GLM1_betaMap_xii)
WB_familiarity_values <- WB_familiarity_values[as.matrix(WB_familiarity_mask) == 1, ]
WB_familiarity_values <- colMeans(WB_familiarity_values)

# Create cluster tables based on the maps
GLM1_cluster1 <- cifti_cluster_report(zMap_file, 
                            clusterMap1_file, 
                            surfLeft, 
                            surfRight,
                            parcellationFile, 
                            minClusterSize,
                            FALSE)

GLM1_cluster2 <- cifti_cluster_report(zMap_file, 
                            clusterMap2_file, 
                            surfLeft, 
                            surfRight,
                            parcellationFile, 
                            minClusterSize,
                            FALSE)

# Round values to in order to report them
GLM1_cluster1$cluster_values$zValue_max   <- signif(GLM1_cluster1$cluster_values$zValue_max, digits1)
GLM1_cluster1$cluster_values$zValue_min   <- signif(GLM1_cluster1$cluster_values$zValue_min, digits1)
GLM1_cluster1$cluster_values$cluster_mass <- signif(GLM1_cluster1$cluster_values$cluster_mass, digits1)

GLM1_cluster2$cluster_values$zValue_max   <- signif(GLM1_cluster2$cluster_values$zValue_max, digits1)
GLM1_cluster2$cluster_values$zValue_min   <- signif(GLM1_cluster2$cluster_values$zValue_min, digits1)
GLM1_cluster2$cluster_values$cluster_mass <- signif(GLM1_cluster2$cluster_values$cluster_mass, digits1)

# Write tsv files so it's easier to edit
## Positive clusters
### Combine to one table
region_labels  <- GLM1_cluster1$cluster_labels[, 4]
combined_table <- cbind(GLM1_cluster1$cluster_values, region_labels)

### Add places between the region labels
combined_table$region_labels <- str_replace_all(combined_table$region_labels, 
                                                pattern = ",",
                                                replacement = ", ")

### Write as .txt file
write.table(combined_table, file = "figures/SpaNov/tables/GLM1_c1.txt", 
            quote = FALSE,
            row.names = FALSE,
            sep = '\t')

## Negative clusters
### Combine to one table
region_labels  <- GLM1_cluster2$cluster_labels[, 4]
combined_table <- cbind(GLM1_cluster2$cluster_values, region_labels)

### Add places between the region labels
combined_table$region_labels <- str_replace_all(combined_table$region_labels, 
                                                pattern = ",",
                                                replacement = ", ")

### Write as .txt file
write.table(combined_table, file = "figures/SpaNov/tables/GLM1_c2.txt", 
            quote = FALSE,
            row.names = FALSE,
            sep = '\t')
```

#### Create unthresholded hippocampal z-map
For unthresholded visualisation of the hippocampal GLM results, we create a version of the z-map that set everything other than the hippocampus to zero. 

```{r create_HC_only_zMaps, eval = FALSE}
# (This chunk only needs to run once, so eval is set to FALSE)
# Extreme comparison: Level 1 vs. Level 6
ciftiFolder <- "/SpaNov/OLMe_7T_SpaNov_gradient_6lvl_cue-delay_smo4_MSMAll/cope14.feat/stats/vwc/"
zMap_file1  <- paste0(path2imaging_results2, ciftiFolder, "results_lvl2cope1_dat_ztstat_c1.dscalar.nii")
zMap_xii1   <- read_cifti(zMap_file1, brainstructures = "all", 
                  surfL_fname = surfLeft, surfR_fname = surfRight)

# Linear contrast from Level 1 to Level 6
ciftiFolder <- "/SpaNov/OLMe_7T_SpaNov_gradient_6lvl_cue-delay_smo4_MSMAll/cope7.feat/stats/vwc/"
zMap_file2  <- paste0(path2imaging_results2, ciftiFolder, "results_lvl2cope1_dat_ztstat_c1.dscalar.nii")
zMap_xii2   <- read_cifti(zMap_file2, brainstructures = "all", 
                  surfL_fname = surfLeft, surfR_fname = surfRight)

# Create mask for everything other than right and left hippocampus
currentMask <- zMap_xii1$meta$subcort$labels != "Hippocampus-R" & 
               zMap_xii1$meta$subcort$labels != "Hippocampus-L"

# Create new variables
zMap_xii1_HC <- zMap_xii1
zMap_xii2_HC <- zMap_xii2

# Set everything within this mask to zero
zMap_xii1_HC$data$subcort[currentMask, ] <- 0
zMap_xii2_HC$data$subcort[currentMask, ] <- 0

# Also set cortical values to zero
zMap_xii1_HC$data$cortex_left  <- matrix(as.integer(0), nrow = 29696, ncol = 1)
zMap_xii1_HC$data$cortex_right <- matrix(as.integer(0), nrow = 29716, ncol = 1)
zMap_xii2_HC$data$cortex_left  <- matrix(as.integer(0), nrow = 29696, ncol = 1)
zMap_xii2_HC$data$cortex_right <- matrix(as.integer(0), nrow = 29716, ncol = 1)

# Create new names
new_zMap_file1 <- str_replace(zMap_file1, pattern = ".dscalar.nii", 
                              replacement = "_onlyHC.dscalar.nii")
new_zMap_file2 <- str_replace(zMap_file2, pattern = ".dscalar.nii", 
                              replacement = "_onlyHC.dscalar.nii")

# Write new cifti files
write_cifti(xifti = zMap_xii1_HC, cifti_fname = new_zMap_file1)
write_cifti(xifti = zMap_xii2_HC, cifti_fname = new_zMap_file2)
```

### Spatial novelty-gradient in hippocampus
#### Calculate GS & tSNR in hippocampus
```{r tSNR_GS_analysis}
# Load tSNR and GS values
load("data/ignore_fMRI_version1/tSNR_GS_maps.RData")

# Add file ID
GS_tSNR_df$rowID <- 1:nrow(GS_tSNR_df)

# Subset to subjects included in the analysis
GS_tSNR_df <- GS_tSNR_df[GS_tSNR_df$GS_subject %in% subjIDs_R, ]

# Calculate average GS & tSNR maps
GS_merged   <- merge_xifti(xifti_list = GS_list[GS_tSNR_df$rowID])
GS_avg      <- apply_xifti(GS_merged, margin = 1, mean)
tSNR_merged <- merge_xifti(xifti_list = tSNR_list[GS_tSNR_df$rowID])
tSNR_avg    <- apply_xifti(tSNR_merged, margin = 1, mean)

# Get MNI coordinates
HC_data_L  <- MNI_coord[str_starts(MNI_coord$region, pattern = "Hippocampus-L"), ]
HC_data_R  <- MNI_coord[str_starts(MNI_coord$region, pattern = "Hippocampus-R"), ]
HC_data    <- rbind(HC_data_L, HC_data_R)

# Get bool index 
HC_index_L <- GS_list[[1]]$meta$subcort$labels == "Hippocampus-L"
HC_index_R <- GS_list[[1]]$meta$subcort$labels == "Hippocampus-R"

# Results list
GS_HC_list   <- list()
tSNR_HC_list <- list()

# Extract hippocampus values by loop over all subjects
for(i in 1:length(subjIDs_R)){
  # GS
  ## Extract values from Run 1 & 2
  run1_xii <- GS_list[[GS_tSNR_df$rowID[GS_tSNR_df$GS_run == "RUN1" & GS_tSNR_df$GS_subject == subjIDs_R[i]]]]
  run2_xii <- GS_list[[GS_tSNR_df$rowID[GS_tSNR_df$GS_run == "RUN2" & GS_tSNR_df$GS_subject == subjIDs_R[i]]]]
  run1_HC  <- c(run1_xii$data$subcort[HC_index_L], run1_xii$data$subcort[HC_index_R])
  run2_HC  <- c(run2_xii$data$subcort[HC_index_L], run2_xii$data$subcort[HC_index_R])
  
  ## Average across runs
  avg_HC   <- (run1_HC + run2_HC)/2
  
  ## Add to data.frame
  temp_df         <- HC_data
  temp_df$subject <- subjIDs_R[i]
  temp_df$GS      <- avg_HC
  GS_HC_list[[i]] <- temp_df
  
  # GS
  ## Extract values from Run 1 & 2
  run1_xii <- tSNR_list[[GS_tSNR_df$rowID[GS_tSNR_df$GS_run == "RUN1" & GS_tSNR_df$GS_subject == subjIDs_R[i]]]]
  run2_xii <- tSNR_list[[GS_tSNR_df$rowID[GS_tSNR_df$GS_run == "RUN2" & GS_tSNR_df$GS_subject == subjIDs_R[i]]]]
  run1_HC  <- c(run1_xii$data$subcort[HC_index_L], run1_xii$data$subcort[HC_index_R])
  run2_HC  <- c(run2_xii$data$subcort[HC_index_L], run2_xii$data$subcort[HC_index_R])
  
  ## Average across runs
  avg_HC   <- (run1_HC + run2_HC)/2
  
  ## Add to data.frame
  temp_df           <- HC_data
  temp_df$subject   <- subjIDs_R[i]
  temp_df$tSNR      <- avg_HC
  tSNR_HC_list[[i]] <- temp_df
}

# Convert to data frame
GS_HC_df   <- rbindlist(GS_HC_list)
tSNR_HC_df <- rbindlist(tSNR_HC_list)
GS_HC_df   <- as.data.frame(GS_HC_df)
tSNR_HC_df <- as.data.frame(tSNR_HC_df)

# Simple A vs. P comparison
## Divide into A & P
GS_HC_df$AP   <- ifelse(GS_HC_df$y > -21, "Anterior", "Posterior")
tSNR_HC_df$AP <- ifelse(tSNR_HC_df$y > -21, "Anterior", "Posterior")

## Calculate average for each participant
GS_HC_df_AP   <- ddply(GS_HC_df, c("subject", "AP"), summarise, GS = mean(GS))
tSNR_HC_df_AP <- ddply(tSNR_HC_df, c("subject", "AP"), summarise, tSNR = mean(tSNR))

## Run tests
### GS
GS_A <- GS_HC_df_AP$GS[GS_HC_df_AP$AP == "Anterior"]
GS_P <- GS_HC_df_AP$GS[GS_HC_df_AP$AP == "Posterior"]
diff <- GS_A - GS_P
GS_d    <- round(mean(diff)/sd(diff), 2)
GS_p    <- t.test(diff)$p.value
GS_BF10 <- signif(reportBF(ttestBF(diff)), 3)

### tSNR
tSNR_A <- tSNR_HC_df_AP$tSNR[tSNR_HC_df_AP$AP == "Anterior"]
tSNR_P <- tSNR_HC_df_AP$tSNR[tSNR_HC_df_AP$AP == "Posterior"]
diff      <- tSNR_A - tSNR_P
tSNR_d    <- round(mean(diff)/sd(diff), 2)
tSNR_p    <- t.test(diff)$p.value
tSNR_BF10 <- signif(reportBF(ttestBF(diff)), 3)

# Calculate percentile of averge hippocampus tSNR
tSNR_A_percentile <- round(mean(as.matrix(tSNR_avg) < mean(tSNR_A)), 2)
tSNR_P_percentile <- round(mean(as.matrix(tSNR_avg) < mean(tSNR_P)), 2)

# Get values from the XIFTIs
HC_tSNR_avg <- c(tSNR_avg$data$subcort[HC_index_L], tSNR_avg$data$subcort[HC_index_R])
HC_GS_avg   <- c(GS_avg$data$subcort[HC_index_L], GS_avg$data$subcort[HC_index_R])

## Create a data frame
HC_tSNR_GS_df   <- data.frame(tSNR = HC_tSNR_avg, GS = HC_GS_avg, 
                              region = rep(c("Hippocampus-L", "Hippocampus-R"), 
                                          times = c(sum(HC_index_L), sum(HC_index_R))))
```

#### Permutation analyses
```{r HC_grad}
# Load hippocampal gradient data
load("intermediate_data/SpaNov_gradient_data_cue-delay.RData")

# Add tSNR & GS
HC_data$tSNR <- HC_tSNR_GS_df$tSNR
HC_data$GS   <- HC_tSNR_GS_df$GS
# In 5b1e712, carefully check that the order HC_data and HC_tSNR_GS_df is 
# the same. Specifically I calculated the position values for HC_tSNR_GS_df
# and then checked if mean(HC_data$position == HC_tSNR_GS_df$position) == 1

# Average across the AP position
HC_data_agg_pos <- ddply(HC_data, c("position", "Hemisphere"), summarise, 
                         n = length(min), min = mean(min), max = mean(max), 
                         tSNR = mean(tSNR), GS = mean(GS))

# Average across position and hemisphere
HC_data_agg_pos2 <- ddply(HC_data, c("position"), summarise, 
                          n = length(min), min = mean(min), max = mean(max),
                          tSNR = mean(tSNR), GS = mean(GS))

# Labels
conN       <- 6
novLabels  <- paste0('Level ', 1:conN)
```

```{r permutation_function}
permutation_analysis <- function(data, lm_formula, nIter, colName, imageName){
  # Initialise results list
  results <- list()
  
  # Select correct column for analysis and create new data frame
  data$val     <- data[, colName]
  data2shuffle <- data
  
  # Calculate empirical values and save to list
  results$lm <- lm(lm_formula, data = data)
  numCoef    <- length(results$lm$coefficients) - 1 # Ignoring the intercept
  
  # Start cluster
  my.cluster <- parallel::makeCluster(detectCores() - 2, type = "PSOCK")
  
  #register it to be used by %dopar%
  doParallel::registerDoParallel(cl = my.cluster)
  
  # Run parallel loop
  permuted_values <- foreach(i = 1:nIter, .combine = 'c', .packages = 'plyr') %dopar% {
    data2shuffle$val <- sample(data$val)
    
    # Fit model
    temp_lm <- lm(lm_formula, data = data2shuffle)
    
    # Add values 
    temp_est <- as.data.frame(matrix(as.numeric(temp_lm$coefficients)[-1], ncol = numCoef))
    names(temp_est) <- names(results$lm$coefficients)[-1]
    list(temp_est)
  }
  
  # Stop cluster again
  parallel::stopCluster(cl = my.cluster) 
  
  # Add to results
  results$permuted_values <- as.data.frame(rbindlist(permuted_values))
  
  # Save to disk
  if(!missing(imageName)){
    save(results, file = paste0("intermediate_data/", imageName))
  }
  
  # Return value
  return(results)
}
```

##### Minimum
```{r grad_min_run_check1}
# Rename to be unique
grad_HC1 <- HC_data_agg_pos

# Check if the code needs to be run again. This is done by checking the md5 has 
# for the data frame that is used in the calculation if it is different from the 
# hash sum saved in md5_hash_table.csv, it is re-run. This is to avoid having to 
# re-run everything each time I work on the data. 
runCodeAgain1 <- check_if_md5_hash_changed(grad_HC1, hash_table_name = "SpaNov_md5_hash_table.csv")
```

###### Model with interaction with hemisphere
```{r HC_grad_min_permute_with_interaction}
# Seed
set.seed(19911225)

# Other input
lm_formula    <- "val ~ position * Hemisphere + tSNR + GS"
nIter         <- 100000
colName       <- "min"
imageName     <- "SpaNov_permut_HC_grad_analysis1_cue-delay.RData"

# Run if necessary
if(runCodeAgain1){
  grad_min_permut1 <- permutation_analysis(grad_HC1, lm_formula, 
                                       nIter, colName, imageName)
} else {
  load(paste0("intermediate_data/", imageName))
  grad_min_permut1 <- results
}

# Select values for plotting
dist                <- grad_min_permut1$permuted_values[,5]
critVal             <- grad_min_permut1$lm$coefficients[6]
grad_min_permut1_p5 <- pValue_from_nullDist(critVal, dist, "two.sided")
```

###### Left hippocampus
```{r HC_grad_min_left}
# Seed
set.seed(19911225)

# Other input
lm_formula    <- "val ~ position + tSNR + GS"
nIter         <- 100000
colName       <- "min"
imageName     <- "SpaNov_permut_HC_grad_analysis1_cue-delay_L.RData"

# Run if necessary
if(runCodeAgain1){
  grad_min_permut1_L <- permutation_analysis(grad_HC1[grad_HC1$Hemisphere == 'left', ], lm_formula, 
                                       nIter, colName, imageName)
} else {
  load(paste0("intermediate_data/", imageName))
  grad_min_permut1_L <- results
}

# Select values for plotting
dist                  <- grad_min_permut1_L$permuted_values[,1]
critVal               <- grad_min_permut1_L$lm$coefficients[2]
grad_min_permut1_L_p1 <- pValue_from_nullDist(critVal, dist, "two.sided")
```

###### Right hippocampus
```{r HC_grad_min_right}
# Seed
set.seed(19911225)

# Other input
lm_formula    <- "val ~ position + tSNR + GS"
nIter         <- 100000
colName       <- "min"
imageName     <- "SpaNov_permut_HC_grad_analysis1_cue-delay_R.RData"

# Run if necessary
if(runCodeAgain1){
  grad_min_permut1_R <- permutation_analysis(grad_HC1[grad_HC1$Hemisphere == 'right', ], lm_formula, 
                                       nIter, colName, imageName)
} else {
  load(paste0("intermediate_data/", imageName))
  grad_min_permut1_R <- results
}

# Select values for plotting
dist                  <- grad_min_permut1_R$permuted_values[,1]
critVal               <- grad_min_permut1_R$lm$coefficients[2]
grad_min_permut1_R_p1 <- pValue_from_nullDist(critVal, dist, "two.sided")
```

##### Maximum
###### Model with interaction with hemisphere
```{r grad_max_permute1}
# Seed
set.seed(19911225)

# Other input
lm_formula    <- "val ~ position * Hemisphere + tSNR + GS"
nIter         <- 100000
colName       <- "max"
imageName    <- "SpaNov_permut_HC_grad_analysis3_cue-delay.RData"

# Run if necessary
if(runCodeAgain1){
  grad_max_permut1 <- permutation_analysis(grad_HC1, lm_formula, 
                                       nIter, colName, imageName)
} else {
  load(paste0("intermediate_data/", imageName))
  grad_max_permut1 <- results
}

# Select values for plotting
dist                <- grad_max_permut1$permuted_values[,5]
critVal             <- grad_max_permut1$lm$coefficients[6]
grad_max_permut1_p5 <- pValue_from_nullDist(critVal, dist, "two.sided")
```

###### Average across hemispheres
```{r grad_max_permute1_avg}
# Rename to be unique
grad_HC2 <- HC_data_agg_pos2

# Check if the code needs to be run again. This is done by checking the md5 has 
# for the data frame that is used in the calculation if it is different from the 
# hash sum saved in md5_hash_table.csv, it is re-run. This is to avoid having to 
# re-run everything each time I work on the data. 
runCodeAgain2 <- check_if_md5_hash_changed(grad_HC2, hash_table_name = "SpaNov_md5_hash_table.csv")

# Seed
set.seed(20131)

# Other input
lm_formula    <- "val ~ position + tSNR + GS"
nIter         <- 100000
colName       <- "max"
imageName    <- "SpaNov_permut_HC_grad_analysis4_cue-delay.RData"

# Run if necessary
if(runCodeAgain2){
  grad_max_permut1_avg <- permutation_analysis(grad_HC2, lm_formula, 
                                       nIter, colName, imageName)
} else {
  load(paste0("intermediate_data/", imageName))
  grad_max_permut1_avg <- results
}

# Select values for plotting
dist              <- grad_max_permut1_avg$permuted_values[,1]
critVal           <- grad_max_permut1_avg$lm$coefficients[2]
grad_max_permut1_avg_p1 <- pValue_from_nullDist(critVal, dist, "two.sided")
```

#### Report stat of permutation analysis
```{r create_table_with_all_HC results}
# Calculate effect sizes
grad_min_permut1_eff     <- eta_squared(car::Anova(grad_min_permut1$lm, type = 2))
grad_min_permut1_L_eff   <- eta_squared(car::Anova(grad_min_permut1_L$lm, type = 2))
grad_min_permut1_R_eff   <- eta_squared(car::Anova(grad_min_permut1_R$lm, type = 2))
grad_max_permut1_R_eff   <- eta_squared(car::Anova(grad_max_permut1$lm, type = 2))
grad_max_permut1_avg_eff <- eta_squared(car::Anova(grad_max_permut1_avg$lm, type = 2))

# Create data frame
## Create variables
tmp_formula   <- c("min ~ position * Hemisphere + tSNR + GS", "min (left) ~ position + tSNR + GS",
                   "min (right) ~ position + tSNR + GS", "max ~ position * Hemisphere + tSNR + GS",
                   "max (avg) ~ position + tSNR + GS")
tmp_coef_name <- c("interaction", "position", "position", "interaction", "position")
tmp_coef_val  <- c(grad_min_permut1$lm$coefficients[6], grad_min_permut1_L$lm$coefficients[2],
                   grad_min_permut1_R$lm$coefficients[2], grad_max_permut1$lm$coefficients[6],
                   grad_max_permut1_avg$lm$coefficients[2])
tmp_coef_es   <- c(grad_min_permut1_eff$Eta2_partial[5], grad_min_permut1_L_eff$Eta2_partial[1],
                   grad_min_permut1_R_eff$Eta2_partial[1], grad_max_permut1_R_eff$Eta2_partial[5],
                   grad_max_permut1_avg_eff$Eta2_partial[1])
tmp_coef_p    <- c(grad_min_permut1_p5, grad_min_permut1_L_p1, grad_min_permut1_R_p1, 
                   grad_max_permut1_p5, grad_max_permut1_avg_p1)

## Convert to numeric values and flip
tmp_coef_val <- -as.numeric(tmp_coef_val)

## Add to one data frame
tab_df <- data.frame(Formula = tmp_formula, Coefficient = tmp_coef_name,
                     beta = tmp_coef_val, eta_squared = tmp_coef_es, p = tmp_coef_p)

## Round columns
tab_df$beta        <- round(tab_df$beta, 3)
tab_df$eta_squared <- round(tab_df$eta_squared, 3)

## Create p-values by looping over all values
for(i in seq_along(tab_df$p)){
  tab_df$p[i] <-paste("p", pValue(as.numeric(tab_df$p[i])))
}

# Show table
kable(tab_df)
```

#### Indidiviudal slopes
```{r HC_gradient_individual_slopes}
# Load individual slopes
load("data/ignore_fMRI_version1/extracted_values/SpavNov_gradient_subject-slopes_20250721_073753.RData")

# Add significance
subjSlopes$sig <- subjSlopes$`Pr(>|t|)` < .05

# Individual slopes from gradient analysis
gradient_subject_slopes_hemisphere     <- -subjSlopes$Estimate[subjSlopes$type == "position" & subjSlopes$effect == "position:Hemisphereright"]
gradient_subject_slopes_position_L     <- -subjSlopes$Estimate[subjSlopes$type == "position_L" & subjSlopes$effect == "position"]
gradient_subject_slopes_position_R     <- -subjSlopes$Estimate[subjSlopes$type == "position_R" & subjSlopes$effect == "position"]
gradient_subject_slopes_position_L_sig <- subjSlopes$sig[subjSlopes$type == "position_L" & subjSlopes$effect == "position"]
gradient_subject_slopes_position_R_sig <- subjSlopes$sig[subjSlopes$type == "position_R" & subjSlopes$effect == "position"]

# Effect of hemispheres
x      <- gradient_subject_slopes_hemisphere
d1     <- signif(mean(x)/sd(x), digits1)
str1   <- mean_SD_str2(x, report_type, digits1, rounding_type)
BF10_1 <- reportBF(ttestBF(x))

# Effect in left hemisphere
x      <- gradient_subject_slopes_position_L
d3     <- signif(mean(x)/sd(x), digits1)
str3   <- mean_SD_str2(x, report_type, digits1, rounding_type)
BF10_3 <- reportBF(ttestBF(x))

# Effect in right hemisphere
x      <- gradient_subject_slopes_position_R
d4     <- signif(mean(x)/sd(x), digits1)
str4   <- mean_SD_str2(x, report_type, digits1, rounding_type)
BF10_4 <- reportBF(ttestBF(x))
```

In the right hippocampus, individual slopes were different from zero,`r str4`, BF10 = `r BF10_4`, d = `r d4`, while they was weak evidence for that null hypothesis in the left hippocampus,`r str3`, BF10 = `r BF10_3`, d = `r d3`. However, evidence for a difference between hemisphere in terms of individual slopes was inconclusive,`r str1`, BF10 = `r BF10_1`, d = `r d1`.

### Figure 2
```{r figure2}
# Create new data frame just for plotting
HC_data_plotting <- HC_data_agg_pos
HC_data_plotting$Hemisphere2 <- ifelse(HC_data_plotting$Hemisphere == "right",
                                       "Right hippocampus", "Left hippocampus")

# Write ,csv file
write.csv(x = HC_data_plotting, file = "other_stuff/source_data_files/HC_scatter_plot.csv", 
          quote = FALSE, row.names = FALSE)

# Create Figure 2
## Create minimum scatter plot for both hemispheres
scatter_min <- ggplot(HC_data_plotting, aes(x = -position, y = min)) + 
  facet_grid(~Hemisphere2, scales = "free_x") + 
  geom_point(aes(colour = min)) +
  geom_smooth(method = "lm", formula = y ~ x, colour = "black") +
  labs(x = "Distance to most anterior part (mm)", y = "Novelty\npreference") + 
  scale_x_continuous(breaks =  seq(from = -45, to = 0, by = 15), labels = c("45", "30", "15", "0")) +
  scale_y_continuous(breaks = 1:6, labels = paste("Lvl", 1:6), limits = c(1, 6)) +
  scale_colour_viridis_c(option = "H", limits = c(1, 6), direction = -1) +
  theme_journal() +
  theme(strip.background = element_rect(color="white", fill="white"),
        axis.title.y = element_text(margin = margin(t = 0, r = 8, b = 0, l = 0)),
        panel.spacing.x = unit(7.5, "mm")) +
  theme(legend.position = "none")

## Create plots for individual slopes
### Create data frame for plotting
individual_slope_df <- data.frame(Effect = rep(c("Right hippocampus", "Left hippocampus", "Interaction"), each = 56),
                                  Coefficient = c(gradient_subject_slopes_position_R,
                                                  gradient_subject_slopes_position_L,
                                                  gradient_subject_slopes_hemisphere))

# Write ,csv file
write.csv(x = individual_slope_df, file = "other_stuff/source_data_files/HC_individual_slopes.csv", 
          quote = FALSE, row.names = FALSE)

### Create plot
individual_slopes_plot <- ggplot(individual_slope_df, aes(x = Effect, y = Coefficient)) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_jitter(colour = boxplot_point_col, width = 0.25) +
  geom_boxplot(colour = boxplot_border_col, alpha = 0.7, outlier.shape = NA, width = 0.4,
               fill = base_col[2]) +
  stat_summary(geom = "point", fun = "mean", col = 'white', shape = 24,
               fill = base_col[2], position = position_dodge(width =  0.75), size = 0.8) +
  theme_journal() + 
  theme(axis.title.y = element_text(margin = margin(t = 0, r = 4.5, b = 0, l = 0))) +
  labs(title = "", x = "", y = "Individual\nregression coefficients")

## Combine both plots
p <- plot_grid(scatter_min, individual_slopes_plot, ncol = 1, align = "hv", axis = "tblr")

## Save
ggsave(p, filename = paste0(figurePath, "Fig_2c_scatter.png"),
       dpi = dpi,  width = 99, height = 83.5, units = "mm")
```

![](figures/SpaNov/Fig_2c_scatter.png)

## Extension of spatial novelty gradient to the posterior medial cortex
### Figure 3
#### Yeo 7 parcellation ridge plot
```{r GLM1_Yeo7_ridges_plot}
# Get the names of the networks (-1 to remove the first row that just contains ???)
Yeo7_labels        <- Yeo7_xii$meta$cifti$labels$parcels[-1, ]
Yeo7_labels$Region <- row.names(Yeo7_labels)
row.names(Yeo7_labels)   <- NULL

# Convert the RGB values to hexcodes
Yeo7_labels$hexcol <- rgb(Yeo7_labels$Red, Yeo7_labels$Green, Yeo7_labels$Blue, 
                          maxColorValue = 1)

# Create basic data frame
Yeo_zValues_df <- data.frame(zValue = c(GLM1_zMap_xii$data$cortex_left, GLM1_zMap_xii$data$cortex_right),
                             Yeo7   = c(Yeo7_xii$data$cortex_left, Yeo7_xii$data$cortex_right))

# Add network label
Yeo_zValues_df$net_label <- NA
for(i in 1:nrow(Yeo7_labels)){
  # Select all rows in Yeo_zValues_df that have this key
  bool_index <- Yeo_zValues_df$Yeo7 == Yeo7_labels$Key[i]
  Yeo_zValues_df$net_label[bool_index] <- Yeo7_labels$Region[i]
}

# Find lowest z value that's significant
## Get the p-values and the z-values
GLM1_pValues1 <- get_all_points_from_xifti(GLM1_pMap1_xii)
GLM1_pValues2 <- get_all_points_from_xifti(GLM1_pMap2_xii)
GLM1_zValues  <- get_all_points_from_xifti(GLM1_zMap_xii)

## Subset to only significant values
GLM1_zValues1 <- GLM1_zValues[GLM1_pValues1 > cutOff]
GLM1_zValues2 <- GLM1_zValues[GLM1_pValues2 > cutOff]

# Exclude vertices that are not included in Yeo 7
Yeo_zValues_df_sub <- Yeo_zValues_df[Yeo_zValues_df$Yeo7 != 0, ]

# Order according to the mean of the network
Yeo_zValues_df_agg <- ddply(Yeo_zValues_df_sub, c("net_label"), summarise, 
                            avg_z = mean(zValue))

## First order alphabetically so we can apply the same order to GLM1_Yeo7
Yeo_zValues_df_agg <- Yeo_zValues_df_agg[order(as.character(Yeo_zValues_df_agg$net_label)), ]

## Now order based on the mean
mean_order         <- order(Yeo_zValues_df_agg$avg_z)
Yeo_zValues_df_agg <- Yeo_zValues_df_agg[mean_order, ]

# Order the colours in the same way
Yeo7_labels <- Yeo7_labels[order(as.character(Yeo7_labels$Region)), ]
Yeo7_labels <- Yeo7_labels[mean_order, ]

# Add the full names
Yeo7_labels$fullNames <- find_values_thru_matching(Yeo7_abbr, Yeo7_fullNames, Yeo7_labels$Region)

# Make net_label a factor with the correct order
Yeo_zValues_df_sub$net_label <- factor(Yeo_zValues_df_sub$net_label, 
                                       levels = Yeo_zValues_df_agg$net_label,
                                       labels = Yeo7_labels$fullNames,
                                       ordered = TRUE)

# Create plot
p1 <- ggplot(Yeo_zValues_df_sub, aes(x = zValue, y = net_label, fill = net_label)) + 
  geom_density_ridges(linewidth = 0.3) +
  scale_fill_manual(values = Yeo7_labels$hexcol) +
  geom_vline(xintercept = max(GLM1_zValues2), linetype = 2, linewidth = 0.3) +
  geom_vline(xintercept = min(GLM1_zValues1), linetype = 2, linewidth = 0.3) +
  theme_journal() + 
  coord_cartesian(ylim = c(0.5, 8.5)) +
  theme(legend.position = "none", plot.margin = unit(c(1,1,1,1), "mm")) +
  labs(x = "z-statistic", y = "")

ggsave(p1,
       filename = paste0(figurePath, "Fig_3a_Yeo7_ridge.png"), 
       dpi = dpi,
       width = 60,
       height = 30,
       units = "mm")
```

#### Margulies' connectivity gradients
```{r Margulies_gradients}
# Load all 10 gradients from brainstat
# Source: https://brainstat.readthedocs.io/en/master/index.html
nGrad          <- 10
prefix         <- "data/ignore_fMRI_version1/brainstat/Gradient_"

# Get the medial walls from donour
mwm_L <- Yeo7_xii$meta$cortex$medial_wall_mask$left
mwm_R <- Yeo7_xii$meta$cortex$medial_wall_mask$right

# Create data frame with 
Margulies_grad        <- as.data.frame(matrix(NA, nrow = 59412, ncol = 10))
names(Margulies_grad) <- paste0("Grad_", 1:10)

# Loop through all gradients
for(i in 1:nGrad){
  # Create a new xifti by reading the gifti files
  tmp_xii <- read_xifti2(cortexL = paste0(prefix, i, "_L.shape.gii"), 
                                       surfL = surfLeft,
                                       mwall_values = c(0),
                                       cortexR = paste0(prefix, i, "_R.shape.gii"),
                                       surfR = surfRight)
  
  # Use the medial wall values from donour and apply them to the loaded file. 
  tmp_xii$data$cortex_left[!mwm_L]  <- NA
  tmp_xii$data$cortex_right[!mwm_R] <- NA
  tmp_xii <- move_to_mwall(tmp_xii, values = c(NA))
  
  # Add to prepared data frame
  Margulies_grad[, i] <- c(tmp_xii$data$cortex_left, tmp_xii$data$cortex_right)
}

# Add Yeo 7 to the data frame
Margulies_grad$Yeo7_name   <- Yeo_zValues_df$net_label

# Remove the missing values
Margulies_grad_sub <- Margulies_grad[!is.na(Margulies_grad$Yeo7_name), ]

# Plot 
p1 <- ggplot(Margulies_grad_sub, aes(x = Grad_2, y = Grad_1, colour = Yeo7_name)) + 
  geom_point(alpha = 0.2, size = 0.3, shape = 16) +
  theme_void() + 
  theme(legend.position = "none") +
  scale_colour_manual(values = Yeo7_colours)

# Save file
ggsave(p1,
       filename = paste0(figurePath, "Fig_3a_Yeo7_Margulies_space.png"), 
       dpi = dpi,
       width = 30,
       height = 30,
       units = "mm")

# Significant vertices for GLM1
Margulies_grad$GLM_1_direction <- NA
Margulies_grad$GLM_1_direction[c(GLM1_pMap1_xii$data$cortex_left, GLM1_pMap1_xii$data$cortex_right) > cutOff]  <- "Familiarity"
Margulies_grad$GLM_1_direction[c(GLM1_pMap2_xii$data$cortex_left, GLM1_pMap2_xii$data$cortex_right) > cutOff]  <- "Novelty"

# Create subset of only significant vertices
Margulies_grad_sub <- Margulies_grad[!is.na(Margulies_grad$GLM_1_direction), ]

# Plot 
p1 <- ggplot(Margulies_grad_sub, aes(x = Grad_2, y = Grad_1, colour = GLM_1_direction)) + 
  geom_point(alpha = 0.2, size = 0.3, shape = 16) +
  theme_void() + 
  scale_colour_manual(values = cool_warm_colours) + 
  theme(legend.position = "none") 

# Save file
ggsave(p1,
       filename = paste0(figurePath, "Fig_3a_NovFam_Margulies_space.png"), 
       dpi = dpi,
       width = 70,
       height = 70,
       units = "mm")
```

```{r predicting_effects_SVM}
# Splitting the dataset into the Training set and Test set 
# https://www.geeksforgeeks.org/classifying-data-using-support-vector-machinessvms-in-r/
svm_df <-  data.frame(Grad_1 = Margulies_grad_sub$Grad_1,
                      Grad_2 = Margulies_grad_sub$Grad_2,
                      Modulation = ifelse(Margulies_grad_sub$GLM_1_direction == "Novelty", 1, 0))
# Seed for this analysis
set.seed(123) 

# Split the data into training and test
split        <- sample.split(svm_df$Modulation, SplitRatio = 0.75) 
training_set <- subset(svm_df, split == TRUE) 
test_set     <- subset(svm_df, split == FALSE) 

# Feature scaling 
training_set[-3] <- scale(training_set[-3]) 
test_set[-3]     <- scale(test_set[-3]) 

# Fitting SVM to the Training set 
classifier <- svm(formula = Modulation ~ ., 
                 data = training_set, 
                 type = 'C-classification', 
                 kernel = 'radial') 

# Remove NA
test_set <- na.omit(test_set)

# Predicting the Test set results
y_pred   <- predict(classifier, newdata = test_set) 
cm       <- table(test_set[, 3], y_pred)/ sum(table(test_set[, 3], y_pred))
cm       <- round(cm *100, 2)
accuracy <- mean(test_set[, 3] == y_pred)
```

The SVM-classification accuracy is `r round(accuracy*100, 2)`% for significant novelty vs. familiarity vertices in Margulies' et al. (2015) gradient space.

## Resting-state connectivity between spatial novelty gradients in hippocampus and the posterior parietal cortex
### Figure 4
```{r FRSC_between_gradients}
############ connectivity_schematic
# Create hippocampus gradient bar
SC_gradient_xii    <- read_cifti("cifti_results/SpaNov_gradient_wholebrain_min_cue-delay.dlabel.nii", brainstructures = c("subcort"))

# Create hippocampus only gradient map and write as CIFTI
HC_gradient_xii <- SC_gradient_xii
HC_gradient_xii$data$subcort[!str_detect(SC_gradient_xii$meta$subcort$labels, pattern = "Hippocampus"), 1] <- as.integer(0)
#write_cifti(HC_gradient_xii, cifti_fname = "cifti_results/SpaNov_gradient_HC_min_cue-delay.dlabel.nii")

############# Model results
# Add Marginal results
load("fitted_brms_models/SpaNov_m_conn2.Rdata")

# Rename model & set variable of interest
m   <- m_conn2
voi <- "diagonal"

# Plot means of the conditions
## Use prediction
pred_means <- predictions(m, newdata = datagrid(diagonal = unique, f_gradient_level_cortex = unique, 
                                                f_gradient_level_HC = unique, subject = unique), 
                           by = voi, re_formula = NULL, type = "response")
pred_means <- as.data.frame(pred_means) 

## Get posterior draws
pred_means_draws <-  get_draws(pred_means)

# Plot means for each participant
pred_means_subject <- avg_comparisons(m, variables = voi, by = c("subject"))
pred_means_subject <- as.data.frame(pred_means_subject) 

## Get posterior draws
pred_means_subject_draws <- get_draws(pred_means_subject)
pred_means_subject_draws <- as.data.frame(pred_means_subject_draws)

# Order by mean difference
subject_ordered <- pred_means_subject$subject[order(pred_means_subject$estimate, decreasing = TRUE)]
pred_means_subject_draws$f_subject <- factor(pred_means_subject_draws$subject,
                                                 levels = subject_ordered, ordered = TRUE)

# Plot
# Width: a vector of probabilities to use that determine the widths of the resulting intervals 
# Here that to 95%
pred_means_subject_draws_p <- ggplot(pred_means_subject_draws, aes(y = f_subject, x = -draw, colour = f_subject)) +
  geom_vline(xintercept = 0, linetype = "dashed") + 
  stat_pointinterval(.width = 0.95, linewidth = 0.25, point_size = 0.05) +
  labs(x = "Within novelty - between\nnovelty connectivity", y = "Individuals") +
  theme_journal() +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        plot.subtitle = element_text(hjust = 0.5),
        legend.position = "none")

# Average difference between the conditions
## Calculate marginal average effect is the contrast 0 vs. 1
mae1_respone <- avg_comparisons(m, variables = voi, type = "response", re_formula = NULL)

## Get posterior draws
mae_response_draws <-  get_draws(mae1_respone)

## Flipping sign because it makes more sense
mae_response_draws$draw <- -mae_response_draws$draw

## Create plot
p1_mae_draws <- ggplot(mae_response_draws, aes(x = draw)) +
  geom_vline(xintercept = 0, linetype = "dashed") + 
  stat_halfeye(fill = base_col[2], point_size = 1.5) +
  labs(x = "Within novelty - between\nnovelty connectivity", y = "Density") +
  theme_journal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) 

# Save 
ggsave(plot_grid(p1_mae_draws, pred_means_subject_draws_p, align = "h"),
       filename = paste0(figurePath, "Fig_4_FRSC.png"), 
       dpi = dpi,
       width = 90,
       height = 45,
       units = "mm")
```

Report MAE

```{r FRSC_between_gradients_MAE, echo = FALSE}
str1 <- create_str_from_avg_comparisons(mae1_respone, "SDs")
```

- Average marginal effect `r str1`

# Methods
## Task and virtual environment
### Trial types of the experiment
#### Encoding trial

Average trial lengths:

```{r average_trial_length}
# Subset data to encoding only
encoding_only <- OLM_7T_trial_results[OLM_7T_trial_results$trialType == "encoding", ]

# Calculate trial duration
encoding_only$trial_duration <- encoding_only$end_time - encoding_only$start_time

# Calculate the average for each subject
trial_dur <- ddply(encoding_only, c("subject"), summarise,
                   total_length = sum(trial_duration),
                   run_length = total_length/2,
                   trial_duration = mean(trial_duration),
                   N = length(subject))

# Create report strings
str1   <-  mean_SD_str2(trial_dur$trial_duration, report_type, digits1, rounding_type)
str2   <-  mean_SD_str2(trial_dur$run_length, report_type, digits1, rounding_type)
```

Average trial duration of an encoding trial: `r str1`.

Average encoding run duration: `r str2`.

## Procedure
### Days between sessions
```{r days_between_sessions}
# Load lab notebook with information on each subject
lab_notebook <- read_excel("data/ignore_fMRI_version1/VP00157_PPT_Info_Final.xlsx")

# Subset to only the participants used in this analysis
lab_notebook <- lab_notebook[lab_notebook$`ID Number` %in% subjects, ]

# Extract time between sessions
val <- lab_notebook$`Delay between sessions`

# Create report string
str1   <-  mean_SD_str2(val, report_type, digits1, rounding_type)
```

- Number of days between sessions `r str1`

## MRI acquisition
### Number of volumes
```{r nVols}
# Load the .RData with the values
load("data/ignore_fMRI_version1/extracted_values/SpaNov_nVol_mean.RData")

# Calculate the total number of volumes
nVol_mean$nVol_total <- nVol_mean$nVol1 + nVol_mean$nVol2

# Convert this to minutes
nVol_mean$dataMinutes <- nVol_mean$nVol_total/60

# Create report strings
str1   <-  mean_SD_str2(nVol_mean$nVol1, report_type, digits1, rounding_type)
str2   <-  mean_SD_str2(nVol_mean$nVol2, report_type, digits1, rounding_type)
minTime <- signif(min(nVol_mean$dataMinutes), 3)
```

- Number of volumes Run 1: `r str1`
- Number of volumes Run 2: `r str2`
- Minimum data in minutes: `r minTime`

## Neuroimaging analysis overview
## Standard GLMs
## Gradient analysis
### Hippocampal gradient analysis

Get the number of average voxels per position

```{r voxel_per_position}
voxelNum <- mean_SD_str2(HC_data_agg_pos$n, 1, digits1, rounding_type)
```

The average number of voxels per position is `r voxelNum`

# Supplementary material
## Supplementary notes
### Behavioural models of novelty separately
```{r behav_model_separate}
# Visits
load("fitted_brms_models/SpaNov_m_visits_poisson.Rdata")
mae1 <- avg_comparisons(m_visits_run1, variables = "s_onset_rel", type = "response", re_formula = NULL)
mae2 <- avg_comparisons(m_visits_run2, variables = "s_onset_rel", type = "response", re_formula = NULL)

# Time since
load("fitted_brms_models/SpaNov_m_timeSince.Rdata")
mae3 <- avg_comparisons(m_timeSince_run1, variables = "s_onset_rel", type = "response", re_formula = NULL)
mae4 <- avg_comparisons(m_timeSince_run2, variables = "s_onset_rel", type = "response", re_formula = NULL)

# Make strings
str1 <- create_str_from_avg_comparisons(mae1, "visits")
str2 <- create_str_from_avg_comparisons(mae2, "visits")
str3 <- create_str_from_avg_comparisons(mae3, "seconds")
str4 <- create_str_from_avg_comparisons(mae4, "seconds")
```

Average marginal effects:
- Run 1: `r str1`
- Run 2: `r str2`
- Run 1: `r str3`
- Run 2: `r str4`

## Supplementary tables
### Significant clusters for linear contrast (Level 1 < Level 2 < Level 3 < ...)
<details>
 <summary>Click here for table. </summary>
```{r clusterTable1, echo = FALSE}
# Create tables
kable(GLM1_cluster1$cluster_values)
kable(GLM1_cluster1$cluster_labels)
```
</details>


### Significant clusters for linear contrast (Level 1 > Level 2 > Level 3 > ...)
<details>
 <summary>Click here for table. </summary>
```{r clusterTable2, echo = FALSE}
# Create tables
kable(GLM1_cluster2$cluster_values)
kable(GLM1_cluster2$cluster_labels)
```
</details>

## Supplementary figures
### Learning curves
```{r learning_curve, eval = FALSE}
# Creating these figures takes too long so eval = FALSE
load("fitted_brms_models/SpaNov_m_navTime.Rdata")

# Subset to retrieval trials
OLM_7T_encoding <- OLM_7T_trial_results[OLM_7T_trial_results$trialType == "encoding", ]

# Calculate distance between start and object
x1 <- OLM_7T_encoding$start_x
z1 <- OLM_7T_encoding$start_z
x2 <- OLM_7T_encoding$object_x
z2 <- OLM_7T_encoding$object_z
OLM_7T_encoding$start2obj <- euclideanDistance3D(x1, 1, z1, x2, 1, z2)

# Scale values
OLM_7T_encoding$s_timesObjectPresented <- scale_m0_sd0.5(OLM_7T_encoding$timesObjectPresented)
OLM_7T_encoding$s_start2obj <- scale_m0_sd0.5(OLM_7T_encoding$start2obj)

# Rename model & set variable of interest
m   <- m_navTime3
voi <- "s_timesObjectPresented"

# Get which raw values the scaled values correspond to
raw_and_scaled_values <- ddply(OLM_7T_encoding, c("timesObjectPresented"), summarise,
                          raw = timesObjectPresented[1],
                          scaled = mean(s_timesObjectPresented))

# Calculate marginal average effect is the contrast 0 vs. 1
mae1_navTime_respone <- avg_comparisons(m, variables = voi, type = "response", re_formula = NULL)
#mae1_navTime_link    <- avg_comparisons(m, variables = voi, type = "link", re_formula = NULL)

# Get posterior draws
mae1_navTime_respone_draws <-  get_draws(mae1_navTime_respone)

p1_mae_draws <- ggplot(mae1_navTime_respone_draws, aes(x = draw)) +
  geom_vline(xintercept = 0, linetype = "dashed") + 
  stat_halfeye(fill = base_col[1], point_size = 1.5) +
  labs(x = "Navigation time (seconds)", y = "Density", 
       title = "Marginal average effect\n of number of times presented") +
  theme_journal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) 

# Create estimates for line
grid <- datagrid(s_timesObjectPresented = seq_min_2_max(raw_and_scaled_values$scaled), 
                 subject = unique, s_start2obj = 0, model = m)
pred_line_navTime <- predictions(m, newdata = grid, by = voi, re_formula = NULL, type = "response")
pred_line_navTime <- as.data.frame(pred_line_navTime)

# Create estimates actually existing values
grid <- datagrid(s_timesObjectPresented = unique, subject = unique, s_start2obj = 0, model = m)
pred_actual_navTime <- predictions(m, newdata = grid, by = voi, re_formula = NULL, type = "response")
pred_actual_navTime <- as.data.frame(pred_actual_navTime) 

# Calculate subject-level lines
grid <- datagrid(s_timesObjectPresented = seq_min_2_max(raw_and_scaled_values$scaled), 
                 subject = unique, s_start2obj = 0, model = m)
pred_line_subject_navTime <- predictions(m, newdata = grid, type = "response")
pred_line_subject_navTime <- as.data.frame(pred_line_subject_navTime)

# Make DFs unique
pred_line_navTime1         <- pred_line_navTime
pred_actual_navTime1       <- pred_actual_navTime
pred_line_subject_navTime1 <- pred_line_subject_navTime

# Assemble plot
p1_line <-ggplot() +
  geom_line(data = pred_line_subject_navTime1, mapping = aes(x = s_timesObjectPresented, y = estimate, group = subject),
            colour = "darkgrey") +
  geom_line(data = pred_line_navTime1, mapping = aes(x = s_timesObjectPresented, y = estimate), linewidth = 0.3) +
  geom_point(data = pred_actual_navTime1, mapping = aes(x = s_timesObjectPresented, y = estimate)) +
  geom_errorbar(data = pred_actual_navTime1, mapping = aes(x = s_timesObjectPresented, ymin = conf.low, ymax = conf.high),
                 width = 0.02) +
  scale_x_continuous(breaks = raw_and_scaled_values$scaled, labels = raw_and_scaled_values$raw) +
  theme_journal() +
  labs(x = "Number of times the object was presented", y = "Navigation time\n(seconds)")

###########################################################################_
# Assemble figure
learning_curves <- plot_grid(p1_mae_draws, p1_line,  align = "hv", ncol = 2, 
                             labels = "auto", label_size = 10)

# Save 
ggsave(learning_curves,
       filename = paste0(figurePath, "Fig_S01_learning_curves.png"), 
       dpi = dpi, width = 130, height = 40, units = "mm")
```

![](figures/SpaNov/Fig_S01_learning_curves.png)

#### Report MAEs
```{r learning_curves_MAEs}
# Load the model 
load("fitted_brms_models/SpaNov_m_navTime.Rdata")

# Rename model & set variable of interest
m   <- m_navTime3
voi <- "s_timesObjectPresented"
mae1_navTime_respone <- avg_comparisons(m, variables = voi, type = "response", re_formula = NULL)
str1 <- create_str_from_avg_comparisons(mae1_navTime_respone, "seconds")
```

- Average marginal effect: `str 1`

### Locomotion by novelty
```{r locomotion}
# Load the image from the gradient event file creation
load("event_tables/images/SpaNov_event_file_gradients.RData")

# Create a data frame
ev_info <- rbindlist(condition_info)

# Subset to the analysis with 6 levels and encoding and for the novelty score
ev_info <- ev_info[ev_info$curr_numLvl == 6 & ev_info$runType == "e" & ev_info$type == "noveltyScore", ]

# Calculate average across runs
ev_info_agg <- ddply(ev_info, c("subj", "level"), summarise, 
                     mean_per_tra_arc = mean(mean_per_tra_arc),
                     mean_per_rot_arc = mean(mean_per_rot_arc),
                     mean_per_sta_arc = mean(mean_per_sta_arc))

# Calculate average for each run
ev_info_agg1 <- ddply(ev_info, c("subj", "run"), summarise, 
                     mean_per_tra_arc = mean(mean_per_tra_arc),
                     mean_per_rot_arc = mean(mean_per_rot_arc),
                     mean_per_sta_arc = mean(mean_per_sta_arc))


# Remove middle levels
ev_info_agg <- ev_info_agg[ev_info_agg$level == "lvl1" | ev_info_agg$level == "lvl6", ]

# Create the plots
## Translation
p1 <- ggplot(ev_info_agg, aes(x = level, y = mean_per_tra_arc, fill = level)) +
  geom_line(aes(group = subj), colour = boxplot_line_col) +
  geom_point(colour = boxplot_point_col) +
  geom_boxplot(colour = boxplot_border_col, outlier.shape = NA, width = 0.4) +
  stat_summary(geom = "point", fun = "mean", col = 'white', shape = 24, 
               position = position_dodge(width =  0.75), size = 0.8) +
  scale_fill_manual(values = novFam_gradient[c(1, 6)]) +
  theme_journal() + 
  scale_y_continuous(breaks = c(0.25, 0.82, 1.40)) + 
  coord_cartesian(xlim = c(0.5, 2.5), ylim = c(0.25, 1.40), expand = FALSE) +
  theme(legend.position = "none") +
  labs(title = "Translation", x = "Novelty level", y = "arcsine(percentage)")

## Rotation
p2 <- ggplot(ev_info_agg, aes(x = level, y = mean_per_rot_arc, fill = level)) +
  geom_line(aes(group = subj), colour = boxplot_line_col) +
  geom_point(colour = boxplot_point_col) +
  geom_boxplot(colour = boxplot_border_col, outlier.shape = NA, width = 0.4) +
  stat_summary(geom = "point", fun = "mean", col = 'white', shape = 24, 
               position = position_dodge(width =  0.75), size = 0.8) +
  scale_fill_manual(values = novFam_gradient[c(1, 6)]) +
  theme_journal() + 
  scale_y_continuous(breaks = c(-1.5, -1, -0.58)) + 
  coord_cartesian(xlim = c(0.5, 2.5), ylim = c(-1.5, -0.58), expand = FALSE) +
  theme(legend.position = "none") +
  labs(title = "Rotation", x = "Novelty level", y = "arcsine(percentage)")

## Stationary
p3 <- ggplot(ev_info_agg, aes(x = level, y = mean_per_sta_arc, fill = level)) +
  geom_line(aes(group = subj), colour = boxplot_line_col) +
  geom_point(colour = boxplot_point_col) +
  geom_boxplot(colour = boxplot_border_col, outlier.shape = NA, width = 0.4) +
  stat_summary(geom = "point", fun = "mean", col = 'white', shape = 24, 
               position = position_dodge(width =  0.75), size = 0.8) +
  scale_fill_manual(values = novFam_gradient[c(1, 6)]) +
  theme_journal() + 
  scale_y_continuous(breaks = c(-1.5, -1.10, -0.73)) + 
  coord_cartesian(xlim = c(0.5, 2.5), ylim = c(-1.5, -0.73), expand = FALSE) +
  theme(legend.position = "none") +
  labs(title = "Stationary", x = "Novelty level", y = "arcsine(percentage)")

# Combine and save
p_comb <- plot_grid(p1, p2, p3, ncol = 3, align = "hv")

ggsave(p_comb,
       filename = paste0(figurePath, "Fig_S02_locomotion.png"), 
       dpi = dpi,
       width = 90,
       height = 40,
       units = "mm")

# Calculate the test statistics
## Translation
val1     <- ev_info_agg[ev_info_agg$level == 'lvl1', "mean_per_tra_arc"]
val2     <- ev_info_agg[ev_info_agg$level == 'lvl6', "mean_per_tra_arc"]
diff_val <- val1 - val2
BF1      <- signif(reportBF(ttestBF(diff_val)), digits1)
d1       <- signif(mean(diff_val)/sd(diff_val), digits1)

## Rotation
val1     <- ev_info_agg[ev_info_agg$level == 'lvl1', "mean_per_rot_arc"]
val2     <- ev_info_agg[ev_info_agg$level == 'lvl6', "mean_per_rot_arc"]
diff_val <- val1 - val2
BF2      <- signif(reportBF(ttestBF(diff_val)), digits1)
d2       <- signif(mean(diff_val)/sd(diff_val), digits1)

## Stationary
val1     <- ev_info_agg[ev_info_agg$level == 'lvl1', "mean_per_sta_arc"]
val2     <- ev_info_agg[ev_info_agg$level == 'lvl6', "mean_per_sta_arc"]
diff_val <- val1 - val2
BF3      <- signif(reportBF(ttestBF(diff_val)), digits1)
d3       <- signif(mean(diff_val)/sd(diff_val), digits1)
```

- Translation Level 1 vs. Level 6: BF10 = `r BF1`, d = `r d1`
- Rotation Level 1 vs. Level 6: BF10 = `r BF2`, d = `r d2`
- Stationary Level 1 vs. Level 6: BF10 = `r BF3`, d = `r d3`

![](figures/SpaNov/Fig_S02_locomotion.png)

### Spatial distribution of novelty scores
```{r novelty_spatial_distribution}
# Circle definitions
## Area of the arena
arenaDiameter <- 180
radius1       <- arenaDiameter/2
areaCircle1   <- pi*(radius1^2)

## Area of circle with half the radius
radius2     <- radius1/2
areaCircle2 <- pi*radius2^2

# Draw inner and outer circle
## Outer circle
circle1   <- circleFun(c(0, 0), arenaDiameter, npoints = 1000)
circle1$z <- circle1$y # Rename to avoid error that columns do not match
circle1$y <- NULL

## Inner circle
circle2   <- circleFun(c(0, 0), radius2*2, npoints = 1000)
circle2$z <- circle2$y # Rename to avoid error that columns do not match
circle2$y <- NULL

# Convert list to data frame
event_info_df <- rbindlist(event_info, idcol = "id")
event_info_df <- as.data.frame(event_info_df)

# Subset to gradient level 1
event_info_df_lvl1 <- event_info_df[event_info_df$noveltyScore_gradientLevel == "lvl1" & 
                                    !is.na(event_info_df$noveltyScore_gradientLevel), ]
event_info_df_lvl1 <- event_info_df_lvl1[event_info_df_lvl1$run %in% c(1, 3), ]

spa_nov_dist <- ggplot() +
  # Circles
  geom_polygon(data = circle1, mapping = aes(x = x, y = z), fill = "lightgrey") +
  geom_polygon(data = circle2, mapping = aes(x = x, y = z), fill = "darkgrey") +
  # Lines and text for outer radius
  geom_segment(aes(x = 0, y = -97, xend = radius1, yend = -97)) +
  geom_segment(aes(x = radius1, y = 0, xend = radius1, yend = -97), linetype = "dashed") +
  geom_segment(aes(x = 0, y = 0, xend = 0, yend = -97), linetype = "dashed") +
  annotate("text", x = 45, y = -102, label = c("90 vm"), size = 2) +
  # Lines for inner radius
  geom_segment(aes(x = 0, y = -95, xend = radius2, yend = -95)) +
  geom_segment(aes(x = radius2, y = 0, xend = radius2, yend = -95), linetype = "dashed") +
  annotate("text", x = radius2/2, y = -90, label = c("45 vm"), size = 2) +
  # Points
  geom_point(data = event_info_df_lvl1, mapping = aes(x = pos_x, y = pos_z)) +
  # Arrows and text
  geom_curve(aes(x = -95, y = 70, xend = -50, yend = 50), ncp = 10,  arrow = arrow(length = unit(1, "mm"))) +
  geom_curve(aes(x = 95, y = 70, xend = 30, yend = 0), ncp = 10,  arrow = arrow(length = unit(1, "mm")), curvature = -0.5) +
  annotate("text", x = c(-95, 95), y = c(73, 73), label = c("Periphery", "Centre"), size = 2) +
  # Other settings
  coord_equal(xlim = c(-105, 105)) +
  theme_void() +
  theme(plot.background = element_rect(fill = "white", colour = "white"))

ggsave(spa_nov_dist,
       filename = paste0(figurePath, "Fig_S03_spa_nov_dist.png"), 
       dpi = dpi, width = 80, height = 80, units = "mm")

# Calculate centrality for each level
## Subset to encoding runs & none NA values
event_info_df_encoding <- event_info_df[event_info_df$run %in% c(1, 3), ]
event_info_df_encoding <- na.omit(event_info_df_encoding)

## Calculate distance to centre
event_info_df_encoding$dist2centre <- euclideanDistance3D(0, 1, 0, event_info_df_encoding$pos_x, 1, event_info_df_encoding$pos_z)

## Categorise centrality
event_info_df_encoding$centrality1 <- ifelse(event_info_df_encoding$dist2centre  >= radius2, 'perihery', 'central')

## Calculate average for each level
avg_level_centrality <- ddply(event_info_df_encoding, c("noveltyScore_gradientLevel"), summarise,
                              centrality1_score = mean(centrality1 == "central"))
```

![](figures/SpaNov/Fig_S03_spa_nov_dist.png)

### Minimal influence of centrality on spatial novelty effects
```{r control_for_centrality}
# Loading files
dist2centre_file <- "/media/alex/work/Seafile/imaging_results/SpaNov/OLMe_7T_SpaNov_gradient_dist2centre-corrected_6lvl_smo4_MSMAll/cope7.feat/stats/vwc/results_lvl2cope1_dat_ztstat_c1.dscalar.nii"
SpaNov1_file     <- "/media/alex/work/Seafile/imaging_results/SpaNov/OLMe_7T_SpaNov_gradient_6lvl_cue-delay_smo4_MSMAll/cope7.feat/stats/vwc/results_lvl2cope1_dat_ztstat_c1.dscalar.nii"
dist2centre_xii  <- read_cifti(dist2centre_file)
SpaNov1_xii      <- read_cifti(SpaNov1_file)

# 
df <- data.frame(dist2centre = as.matrix(dist2centre_xii),
                 SpaNov1 = as.matrix(SpaNov1_xii))

# Create correlation string
r_val <- round(cor.test(df$dist2centre, df$SpaNov1)$estimate, 2)
p_val <- pValue(cor.test(df$dist2centre, df$SpaNov1)$p.value)
report_str <- paste0("r = ", r_val, ", p ", p_val)

# Create scatter plot
p_map_corr <- ggplot(df, aes(x = dist2centre, y = SpaNov1)) + 
  geom_density_2d_filled() +
  #geom_smooth(method = "lm", formula = "y ~ x", colour = "white", se = FALSE) + 
  theme_journal() +
  theme(legend.position = "none") +
  theme(plot.margin = unit(c(1, 4, 1, 1), "mm")) +
  coord_cartesian(expand = FALSE) +
  annotate("text", x = I(0.3), y = I(0.9), label = report_str, size = 2, colour = "white") +
  labs(x = "Z-map corrected for centrality", y = "Original z-map")

##############################################################################_
# Calculate centrality
## Area of the arena
radius1     <- 180/2
areaCircle1 <- pi*(radius1^2)

## Area of circle with half the radius
radius2     <- radius1/2
areaCircle2 <- pi*radius2^2

## Subset position data only encoding (include cue & delay because they are the same for all)
centrality_data <- locomotion_data

## Calculate distance to centre
centrality_data$dist2centre <- euclideanDistance3D(0, 1, 0, centrality_data$pos_x, 1, centrality_data$pos_z)

## Categorise centrality
centrality_data$centrality1 <- ifelse(centrality_data$dist2centre  >= radius2, 'perihery', 'central')

## Calculate average
avg_centrality <- ddply(centrality_data, c("ppid","subject"), summarise,
                        score1 = mean(centrality1 == "central"))

## Write .csv for PALM analysis
write.csv(avg_centrality[,-2], file = "data/ignore_fMRI_version1/SpaNov_centrality_designMatrix_data.csv",
          row.names = FALSE, quote = FALSE)

##############################################################################_
p <- plot_grid(p_map_corr, NULL, NULL, 
               nrow = 1, labels = "auto", label_size = 10, align = "hv")

# Combine plots
ggsave(p,
       filename = paste0(figurePath, "Fig_S04_control_for_centrality.png"), 
       dpi = dpi, width = 180, height = 57, units = "mm")
```

### Individual differences in centrality do not predict spatial novelty
```{r individual_differences}
## Calculate average centrality
str1 <- mean_SD_str2(avg_centrality$score1, report_type, digits1, rounding_type)

## Create plot
avg_centrality_plot <- ggplot(avg_centrality, aes(x = score1)) +
  geom_histogram(colour = base_col[1], fill = base_col[1]) +  
  geom_vline(xintercept = mean(avg_centrality$score1), colour = "black", linetype = 2, size = 0.5) +
  scale_x_continuous(breaks = c(0.45, 0.55, 0.65)) + 
  scale_y_continuous(breaks = c(0, 5, 10)) + 
  coord_cartesian(xlim = c(0.44, 0.65), ylim = c(0, 10), 
                  expand = FALSE) +
  labs(x = "Centrality score", y = "Count") +
  theme_journal() +
  theme(plot.margin = unit(c(1, 2.5, 1, 2.5), "mm"))

##############################################################################_
#  Correlate with average beta values
## Add beta values to data frame
avg_centrality$WB_novelty     <- WB_novelty_values
avg_centrality$WB_familiarity <- WB_familiarity_values

## Calculate correlations
r1     <- round(cor(avg_centrality$score1, avg_centrality$WB_novelty), 2)
BF10_1 <- reportBF(correlationBF(avg_centrality$score1, avg_centrality$WB_novelty))
p1     <- pValue(cor.test(avg_centrality$score1, avg_centrality$WB_novelty)$p.value)
r2     <- round(cor(avg_centrality$score1, avg_centrality$WB_familiarity), 2)
BF10_2 <- reportBF(correlationBF(avg_centrality$score1, avg_centrality$WB_familiarity))
p2     <- pValue(cor.test(avg_centrality$score1, avg_centrality$WB_familiarity)$p.value)

scatter1 <- ggplot(avg_centrality, aes(x = score1, y = WB_novelty)) +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x", colour = base_col[1]) +
  #scale_x_continuous(breaks = c(0.50, 0.575, 0.65)) + 
  scale_y_continuous(breaks = c(-400, -200, 0, 200)) + 
  coord_cartesian(xlim = c(0.50, 0.65), ylim = c(-400, 200), 
                  expand = FALSE) +
  theme_journal() +
  labs(x = "Centrality score", y = "Whole-brain novelty\ncontrast (a.u.)")

scatter2 <- ggplot(avg_centrality, aes(x = score1, y = WB_familiarity)) +
  geom_point() +
  geom_smooth(method = "lm", formula = "y ~ x", colour = base_col[1]) +
  scale_y_continuous(breaks = c(-100, 0, 100, 200)) + 
  coord_cartesian(xlim = c(0.50, 0.65), ylim = c(-100, 210), 
                  expand = FALSE) +
  theme_journal() +
  labs(x = "Centrality score", y = "Whole-brain familiarity\ncontrast (a.u.)")

##############################################################################_
p <- plot_grid(avg_centrality_plot, scatter1, scatter2, 
               nrow = 1, labels = "auto", label_size = 10, align = "hv")

# Combine plots
ggsave(p,
       filename = paste0(figurePath, "Fig_S05_individual_differences_centrality.png"), 
       dpi = dpi, width = 180, height = 55, units = "mm")
```

- The correlation between the centrality score and the novelty contrast was r = `r r1`, BF10 = `r BF10_1`, p `r p1`.
- The correlation between the centrality score and the familiarity contrast was r = `r r2`, BF10 = `r BF10_2`, p `r p2`.

```{r correcting_pvalues}
# Load the p-value maps that have been corrected across main effect and correlation
SpaNov1_centrality_p1_file <- "/media/alex/work/Seafile/imaging_results/SpaNov/OLMe_7T_SpaNov_gradient_6lvl_smo4_MSMAll_centrality/cope7.feat/stats/vwc/results_lvl2cope1_dat_ztstat_cfdrp_c3.dscalar.nii"
SpaNov1_centrality_p2_file <- "/media/alex/work/Seafile/imaging_results/SpaNov/OLMe_7T_SpaNov_gradient_6lvl_smo4_MSMAll_centrality/cope7.feat/stats/vwc/results_lvl2cope1_dat_ztstat_cfdrp_c4.dscalar.nii"

# Load the maps
SpaNov1_centrality_p1_xii <- read_cifti(SpaNov1_centrality_p1_file, brainstructures = "all")
SpaNov1_centrality_p2_xii <- read_cifti(SpaNov1_centrality_p2_file, brainstructures = "all")

# Correct again only across the two maps
new_correction <- correct_4_multiple_contrast_xiftis(list(SpaNov1_centrality_p1_xii, SpaNov1_centrality_p2_xii))

# Write to new cifti files
new_name <- str_replace(SpaNov1_centrality_p1_file, pattern = ".dscalar.nii", replacement = "new.dscalar.nii")
write_cifti(new_correction[[1]], cifti_fname = new_name)
new_name <- str_replace(SpaNov1_centrality_p2_file, pattern = ".dscalar.nii", replacement = "new.dscalar.nii")
write_cifti(new_correction[[2]], cifti_fname = new_name)

# Create data frame to compare differences
values <- c(as.matrix(SpaNov1_centrality_p1_xii), as.matrix(SpaNov1_centrality_p2_xii),
            as.matrix(new_correction[[1]]), as.matrix(new_correction[[2]]))
compare_p_dist <- data.frame(dist_type = rep(c("Correcting across main effect", "Only correcting across\npositive and negative"), each = length(values)/2),
                             log_p_values = values)

# Convert from log to actual p-values -log(0.05, 10)
compare_p_dist$p_values <- 10^-(compare_p_dist$log_p_values)

# Get lowest p-value
min_p <- min(compare_p_dist$p_values[compare_p_dist$dist_type == "Only correcting across\npositive and negative"])

# Five lowest p-values
low_5_p <- head(sort(compare_p_dist$p_values[compare_p_dist$dist_type == "Only correcting across\npositive and negative"]))
```

The five lowest p-values are `r low_5_p`.

### Hippocampal gradient based on maximum statistic
```{r HC_grad_max}
# Create supplemental figure for maximum stat
## Creating a new data frame for combined plotting
HC_data_plotting2 <- HC_data_plotting
HC_data_agg_pos2$Hemisphere2 <- "Average"
HC_supp_fig_data <- data.frame(position = c(HC_data_plotting2$position, HC_data_agg_pos2$position),
                               max = c(HC_data_plotting2$max, HC_data_agg_pos2$max),
                               Hemisphere2 = c(HC_data_plotting2$Hemisphere2, HC_data_agg_pos2$Hemisphere2))

## Re-order Hemisphere2 so the order is left, right, then average
HC_supp_fig_data$Hemisphere2 <- factor(HC_supp_fig_data$Hemisphere2, 
                                       levels =  c("Left hippocampus", "Right hippocampus",
                                                   "Average"),
                                       ordered = TRUE)
## Create plot
scatter_max <- ggplot(HC_supp_fig_data, aes(x = -position, y = max)) + 
  facet_grid(~Hemisphere2, scales = "free_x") + 
  geom_point(aes(colour = max)) +
  geom_smooth(method = "lm", formula = y ~ x, colour = "black") +
  labs(x = "Distance to most anterior part (mm)", y = "Novelty Preference") + 
  scale_x_continuous(breaks =  seq(from = -45, to = 0, by = 15), labels = c("45", "30", "15", "0")) +
  scale_y_continuous(breaks = 1:6, labels = paste("Lvl", 1:6), limits = c(0.5, 6.5)) +
  scale_colour_viridis_c(option = "H", limits = c(1, 6), direction = -1) +
  theme_journal() +
  theme(strip.background = element_rect(color="white", fill="white")) +
  theme(legend.position = "none")

## Save with as 80 mm x 80 mm
ggsave(scatter_max, filename = paste0(figurePath, "Fig_S06_max_scatter.png"),
       dpi = dpi,  width = 120, height = 40, units = "mm")
```

### Hippocampal gradient without averaging
```{r HC_grad_no-avg_models}
# Create new copy of HC_data and create check sum
HC_data_no_avg <- HC_data
runCodeAgain3  <- check_if_md5_hash_changed(HC_data_no_avg, hash_table_name = "SpaNov_md5_hash_table.csv")

# Run/load the code/data
if(runCodeAgain3){
  # Seed
  set.seed(21312)
  
  # Other input
  lm_formula    <- "val ~ position * Hemisphere + tSNR + GS"
  nIter         <- 100000
  colName       <- "min"
  imageName     <- "SpaNov_permut_HC_grad_analysis1_cue-delay_no-avg.RData"
  
  grad_min_permut1_no_avg <- permutation_analysis(HC_data, lm_formula, nIter, 
                                                  colName, imageName)
  # Other input
  lm_formula    <- "val ~ position + tSNR + GS"
  nIter         <- 100000
  colName       <- "min"
  imageName     <- "SpaNov_permut_HC_grad_analysis1_cue-delay_L_no-avg.RData"
  
  grad_min_permut1_L_no_avg <- permutation_analysis(HC_data[HC_data$Hemisphere == "left", ], 
                                             lm_formula, nIter, colName, imageName)
  
  # Other input
  lm_formula    <- "val ~ position + tSNR + GS"
  nIter         <- 100000
  colName       <- "min"
  imageName     <- "SpaNov_permut_HC_grad_analysis1_cue-delay_R_no-avg.RData"
  
  grad_min_permut1_R_no_avg <- permutation_analysis(HC_data[HC_data$Hemisphere == 'right', ], 
                                             lm_formula, nIter, colName, imageName)
} else {
  load("intermediate_data/SpaNov_permut_HC_grad_analysis1_cue-delay_no-avg.RData")
  grad_min_permut1_no_avg   <- results
  load("intermediate_data/SpaNov_permut_HC_grad_analysis1_cue-delay_L_no-avg.RData")
  grad_min_permut1_L_no_avg <- results
  load("intermediate_data/SpaNov_permut_HC_grad_analysis1_cue-delay_R_no-avg.RData")
  grad_min_permut1_R_no_avg <- results
}

# Calculate p-value
dist      <- grad_min_permut1_no_avg$permuted_values[,5]
critVal   <- grad_min_permut1_no_avg$lm$coefficients[6]
grad_min_permut1_no_avg_p5 <- pValue_from_nullDist(critVal, dist, "two.sided")

# Calculate p-value
dist     <- grad_min_permut1_L_no_avg$permuted_values[,1]
critVal  <- grad_min_permut1_L_no_avg$lm$coefficients[2]
grad_min_permut1_L_no_avg_p1 <- pValue_from_nullDist(critVal, dist, "two.sided")

# Calculate p-value
dist      <- grad_min_permut1_R_no_avg$permuted_values[,1]
critVal   <- grad_min_permut1_R_no_avg$lm$coefficients[2]
grad_min_permut1_R_no_avg_p1 <- pValue_from_nullDist(critVal, dist, "two.sided")

# Calculate effect sizes
grad_min_permut1_no_avg_eff     <- eta_squared(car::Anova(grad_min_permut1_no_avg$lm, type = 2))
grad_min_permut1_L_no_avg_eff   <- eta_squared(car::Anova(grad_min_permut1_L_no_avg$lm, type = 2))
grad_min_permut1_R_no_avg_eff   <- eta_squared(car::Anova(grad_min_permut1_R_no_avg$lm, type = 2))

# Create data frame
## Create variables
tmp_formula   <- c("min ~ position * Hemisphere + tSNR + GS", "min (left) ~ position + tSNR + GS",
                   "min (right) ~ position + tSNR + GS")
tmp_coef_name <- c("interaction", "position", "position")
tmp_coef_val  <- c(grad_min_permut1_no_avg$lm$coefficients[6], grad_min_permut1_L_no_avg$lm$coefficients[2],
                   grad_min_permut1_R_no_avg$lm$coefficients[2])
tmp_coef_es   <- c(grad_min_permut1_no_avg_eff$Eta2_partial[5], grad_min_permut1_L_no_avg_eff$Eta2_partial[1],
                   grad_min_permut1_R_no_avg_eff$Eta2_partial[1])
tmp_coef_p    <- c(grad_min_permut1_no_avg_p5, grad_min_permut1_L_no_avg_p1, 
                   grad_min_permut1_R_no_avg_p1)

## Convert to numeric values and flip
tmp_coef_val <- -as.numeric(tmp_coef_val)

## Add to one data frame
tab_df <- data.frame(Formula = tmp_formula, Coefficient = tmp_coef_name,
                     beta = tmp_coef_val, eta_squared = tmp_coef_es, p = tmp_coef_p)

## Round columns
tab_df$beta        <- round(tab_df$beta, 3)
tab_df$eta_squared <- round(tab_df$eta_squared, 3)

## Create p-values by looping over all values
for(i in seq_along(tab_df$p)){
  tab_df$p[i] <-paste("p", pValue(as.numeric(tab_df$p[i])))
}

# Show table
kable(tab_df)
```

```{r HC_grad_no-avg_figure}
# Add better names for the hemispheres
HC_data$Hemisphere2 <- ifelse(HC_data$Hemisphere == "right",
                              "Right hippocampus", "Left hippocampus")

# Create plots
scatter_min_no_avg <- ggplot(HC_data, aes(x = -position, y = min)) + 
  facet_grid(~Hemisphere2, scales = "free_x") + 
  geom_jitter(aes(colour = min), height = 0.1, width = 0) +
  geom_smooth(method = "lm", formula = y ~ x, colour = "black") +
  labs(x = "Distance to most anterior part (mm)", y = "Novelty Preference") + 
  scale_x_continuous(breaks =  seq(from = -45, to = 0, by = 15), labels = c("45", "30", "15", "0")) +
  scale_y_continuous(breaks = 1:6, labels = paste("Lvl", 1:6), limits = c(0.5, 6.5)) +
  scale_colour_viridis_c(option = "H", limits = c(1, 6), direction = -1) +
  theme_journal() +
  theme(strip.background = element_rect(color="white", fill="white")) +
  theme(legend.position = "none")

## Save with as 80 mm x 80 mm
ggsave(scatter_min_no_avg, filename = paste0(figurePath, "Fig_S07_scatter_no-avg.png"),
       dpi = dpi,  width = 80, height = 40, units = "mm")
```

### Relationship between spatial memory and successful memory encoding
```{r successful_memory_encoding}
# Calculate the average placement error for each object & subject. 
obj_avg <- ddply(OLM_7T_retrieval, c("subject", "objectName"), summarise,
                 euclideanDistance = mean(euclideanDistance), N = length(subject),
                 navTime = mean(navTime))

# Rank the objects within each subject
obj_avg <- ddply(obj_avg, c("subject"), mutate, rank = order(euclideanDistance))
obj_avg$f_rank <- factor(obj_avg$rank)

# Visualise results
p1 <- ggplot(obj_avg, aes(x = objectName, y = rank, fill = objectName)) + 
  geom_jitter(width = 0.25, height = 0, pch = 21) +
  geom_boxplot(outlier.shape = NA, alpha = 0.8) +
  stat_summary(geom = "point", fun = "mean", col = 'white', size = 0.8, shape = 24, aes(fill = objectName),
               position = position_dodge(width =  0.75),
               key_glyph = "rect") +
  scale_fill_viridis_d(option = "turbo") +
  labs(title = "Rank by objects",
       x = "Objects", y = "Within-subject rank")  +
  theme_journal() +
  theme(legend.position = 'none',
        plot.title = element_text(hjust = 0.5))

p2 <- ggplot(obj_avg, aes(x = f_rank, y = euclideanDistance, fill = f_rank)) + 
  geom_jitter(width = 0.25, height = 0, pch = 21) +
  geom_boxplot(outlier.shape = NA, alpha = 0.8) +
  stat_summary(geom = "point", fun = "mean", col = 'white', size = 0.8, shape = 24, aes(fill = f_rank),
               position = position_dodge(width =  0.75),
               key_glyph = "rect") +
  labs(title = "Placement error by rank",
       x = "Rank", y = "Avg. placement error (vm)")  +
  theme_journal() +
  scale_fill_viridis_d() +
  theme(legend.position = 'none',
        plot.title = element_text(hjust = 0.5))

p3 <- ggplot(obj_avg, aes(x = f_rank, y = navTime, fill = f_rank)) + 
  geom_jitter(width = 0.25, height = 0, pch = 21) +
  geom_boxplot(outlier.shape = NA, alpha = 0.8) +
  stat_summary(geom = "point", fun = "mean", col = 'white', size = 0.8, shape = 24, aes(fill = f_rank),
               position = position_dodge(width =  0.75),
               key_glyph = "rect") +
  labs(title = "Navigation time by rank",
       x = "Rank", y = "Navigation time (seconds)")  +
  theme_journal() +
  scale_fill_viridis_d() +
  theme(legend.position = 'none',
        plot.title = element_text(hjust = 0.5))

###############################################################################_
# Create base path
base_path <- "/media/alex/work/Seafile/imaging_results/SpaNov/OLMe_7T_encoding1_cue-delay_smo4_MSMAll/cope7.feat/stats/vwc/"

# Load the beta values for encoding contrast
encoding_betaMap_xii  <- read_cifti(paste0(base_path, "Y1.dtseries.nii"), brainstructures = "all")

# Get p-value maps
encoding_FDRp_c1_xii <- read_cifti(paste0(base_path, "results_lvl2cope1_dat_ztstat_cfdrp_c1.dscalar.nii"), brainstructures = "all")
encoding_FDRp_c2_xii <- read_cifti(paste0(base_path, "results_lvl2cope1_dat_ztstat_cfdrp_c2.dscalar.nii"), brainstructures = "all")
# Count significant gray-ordinates
# sum(as.matrix(encoding_FDRp_c1_xii) > cutOff) # = 548
# sum(as.matrix(encoding_FDRp_c2_xii) > cutOff) # = 548

# Calculate average successful encoding contrast in novelty masls
encoding_WB_familiarity <- as.matrix(encoding_betaMap_xii)[as.matrix(WB_familiarity_mask) == 1, ]
encoding_WB_familiarity <- colMeans(encoding_WB_familiarity)
encoding_WB_novelty     <- as.matrix(encoding_betaMap_xii)[as.matrix(WB_novelty_mask) == 1, ]
encoding_WB_novelty     <- colMeans(encoding_WB_novelty)

# Calculate stats
x      <- encoding_WB_familiarity
d1     <- signif(mean(x)/sd(x), digits1)
str1   <- mean_SD_str2(x, report_type, digits1, rounding_type)
BF10_1 <- reportBF(ttestBF(x))
x      <- encoding_WB_novelty
d2     <- signif(mean(x)/sd(x), digits1)
str2   <- mean_SD_str2(x, report_type, digits1, rounding_type)
BF10_2 <- reportBF(ttestBF(x))

# Create a data frame for plotting
encoding_WB_df <- data.frame(mask = rep(c("Familiarity", "Novelty"), each = length(encoding_WB_familiarity)),
                             contrast = c(encoding_WB_familiarity, encoding_WB_novelty))

p4 <- ggplot(encoding_WB_df, aes(x = mask, y = contrast, fill = mask)) + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_jitter(width = 0.25, height = 0, pch = 21) +
  geom_boxplot(outlier.shape = NA, alpha = 0.8) +
  stat_summary(geom = "point", fun = "mean", col = 'white', size = 0.8, shape = 24, aes(fill = mask),
               position = position_dodge(width =  0.75),
               key_glyph = "rect") +
  annotate("text", x = 1, y = 200, label = paste("BF10 =", BF10_1), size = 2) +
  annotate("text", x = 2, y = 250, label = paste("BF10 =", BF10_2), size = 2) +
  labs(title = "",
       x = "Mask", y = "Encoding success (a.u.)")  +
  theme_journal() + 
  scale_fill_manual(values = cool_warm_colours) +
  theme(legend.position = 'none',
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        plot.margin = unit(c(0, 1, 0, 22.5), "mm"))

# Assemble the plot
p <- plot_grid(p1, p2, p3, NULL, NULL, p4,
               align = "h", nrow = 2, #labels = c("a", "b", "c", "d", "", "e"), 
               label_size = 10, rel_heights = c(1, 1))
# Save
ggsave(p,
       filename = paste0(figurePath, "Fig_S08_successful_encoding.png"), 
       dpi = dpi,
       width = 180,
       height = 100,
       units = "mm")
```

- Encoding effect in familiarity regions: d = `r d1`, `r str1`, BF10 = `r BF10_1`
- Encoding effect in novelty regions: d = `r d2`, `r str2`, BF10 = `r BF10_2`

### Spatial novelty gradient in posterior medial cortex showed different principal functional connectivity manifold profiles
```{r PMC_Margulies}
# Compare the PMC gradient with the first connectivity gradient from Margulies et al. (2016)
## Load PMC gradient
PMC_gradient <- read_cifti("cifti_results/SpaNov_gradient_PMC_min_cue-delay.dlabel.nii")

## Add to Margulies_grad data frame
Margulies_grad$PMC_gradient <- c(PMC_gradient$data$cortex_left, PMC_gradient$data$cortex_right)

## Subset to only vertices inside the PMC gradient
Margulies_grad_PMC <- Margulies_grad[Margulies_grad$PMC_gradient != 0, ]

## Make factor
Margulies_grad_PMC$PMC_gradient <- factor(Margulies_grad_PMC$PMC_gradient, levels = 1:6, ordered = TRUE)

## Remove Level5 because there are only 3 vertices
Margulies_grad_PMC <- Margulies_grad_PMC[Margulies_grad_PMC$PMC_gradient != 5, ]

## Create a figure
Margulies_novelty <- ggplot(Margulies_grad_PMC, aes(x = PMC_gradient, y = Grad_1, fill = PMC_gradient)) +
  geom_jitter(height = 0, size = 0.5, alpha = 0.1, width = 0.2) +
  geom_boxplot(alpha = 0.8, outlier.shape = NA) +
  stat_summary(geom = "point", fun = "mean", col = 'white', shape = 24, 
               position = position_dodge(width =  0.75), size = 0.8) +
  scale_fill_manual(values = novFam_gradient[-5]) +
  theme_journal() + 
  theme(legend.position = "none") +
  labs(x = "Spatial novelty-familarity gradient (PMC)", y = TeX(r"( $Margulies'\ et\ al^{10}\ 1^{st}\ gradient$ )"))

# Save
ggsave(Margulies_novelty,
       filename = paste0(figurePath, "Fig_S09_PMC_gradient_Margulies.png"), 
       dpi = dpi,
       width = 80,
       height = 60,
       units = "mm")
```

### Hippocampal connectivity patterns correlating with Margulies gradient
```{r HC_connectivity_and_Margulies}
# Load results
load("data/ignore_fMRI_version1/grad_FRSC/HC_2_cortex_FRSC_Margulies_gradient.RData")

# Apply fisher z-transformation
HC_2_cortex_Margulies_gradient$z_correlation <-  z_transform_fisher(HC_2_cortex_Margulies_gradient$correlation)

# Add MNI y coordinate to it 
HC_2_cortex_Margulies_gradient$MNI_y <- rep(rep(HC_data$y, 10), 56)

# Divide into anterior and poster
HC_2_cortex_Margulies_gradient$AP   <- ifelse(HC_2_cortex_Margulies_gradient$MNI_y >= -21, 
                                              "Anterior", "Posterior")
# Calculate average
HC_2_Margulies_agg1 <- ddply(HC_2_cortex_Margulies_gradient, c("Rnum", "Gradient", "AP"),
                            summarise, z_correlation = mean(z_correlation))

# Make gradient a factor
HC_2_Margulies_agg1$Gradient <- factor(HC_2_Margulies_agg1$Gradient)

# Calculate differences
HC_2_Margulies_agg2 <- ddply(HC_2_Margulies_agg1, c("Rnum", "Gradient"),
                            summarise, diff = mean(z_correlation[1] - z_correlation[2]))

# Calculate tests for each gradient
BF10 <- rep(NA, 10)
d    <- rep(NA, 10)

for(i in 1:10){
  # Calculate one sample difference
  diff <- HC_2_Margulies_agg2$diff[HC_2_Margulies_agg2$Gradient == i]
  d[i] <- mean(diff)/sd(diff)
  BF10[i] <- signif(reportBF(ttestBF(diff)), 2)
}

# Make it a data frame for simpler adding
annot_df <- data.frame(label = paste("BF10 =", BF10),
                       Gradient = factor(1:10),
                       y = rep(c(0.09, 0.08), 5))

# Visualise
HC_2_margulies <- ggplot(HC_2_Margulies_agg2, aes(x = Gradient, y = diff, fill = Gradient)) +
  geom_hline(yintercept = 0, linetype = "dashed", colour = "darkgrey") +
  scale_fill_viridis_d() +
  geom_boxplot(outlier.shape = NA) + 
  stat_summary(geom = "point", fun = "mean", col = 'white', shape = 24, 
               position = position_dodge(width =  0.75), size = 0.8) +
  annotate("text", x = annot_df$Gradient, y = annot_df$y, label = annot_df$label, size = 1.5) +
  theme_journal() +
  theme(legend.position = "none", plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) +
  labs(x = "Principal gradient", y = "Anterior - posterior difference\nin correlation")

# Save
ggsave(HC_2_margulies,
       filename = paste0(figurePath, "Fig_S10_HC_2_Margulies.png"), 
       dpi = dpi,
       width = 120,
       height = 60,
       units = "mm")
```

### Posterior predictive checks for the novelty score model
Find information here: https://mc-stan.org/rstanarm/reference/pp_check.stanreg.html

```{r ppc}
# Set extra seed because these plot are relatively variable with regard to the tails
set.seed(20240206)

p1 <- brms::pp_check(m_noveltyScore_run1, ndraws = 100) + 
  scale_colour_manual(values = c("black", base_col[1]), name = "") +
  coord_cartesian(xlim = c(-20, 20), expand = TRUE) + 
  theme_journal() + 
  theme(axis.line.y = element_blank(),
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),
        legend.position = "none") 

p2 <- brms::pp_check(m_noveltyScore_run2, ndraws = 100) + 
  scale_colour_manual(values = c("black", base_col[1]), name = "") +
  coord_cartesian(xlim = c(-50, 50), expand = TRUE) + 
  theme_journal() + 
  theme(axis.line.y = element_blank(),
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),
        legend.position = "none") 

p3 <- brms::pp_check(m_conn2, ndraws = 100) + 
  scale_colour_manual(values = c("black", base_col[1]), name = "") +
  coord_cartesian(xlim = c(-10, 10), expand = TRUE) + 
  theme_journal() + 
  theme(axis.line.y = element_blank(),
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),
        legend.position = "none") 

p4 <- brms::pp_check(m_navTime3, ndraws = 100) + 
  scale_colour_manual(values = c("black", base_col[1]), name = "") +
  coord_cartesian(xlim = c(0, 60), expand = TRUE) + 
  theme_journal() + 
  theme(axis.line.y = element_blank(),
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),
        legend.position = "none") 

p6 <- brms::pp_check(m_visits_run1, ndraws = 100) + 
  scale_colour_manual(values = c("black", base_col[1]), name = "") +
  coord_cartesian(xlim = c(0, 20), expand = TRUE) + 
  theme_journal() + 
  theme(axis.line.y = element_blank(),
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),
        legend.position = "none") 

p7 <- brms::pp_check(m_visits_run2, ndraws = 100) + 
  scale_colour_manual(values = c("black", base_col[1]), name = "") +
  coord_cartesian(xlim = c(0, 20), expand = TRUE) + 
  theme_journal() + 
  theme(axis.line.y = element_blank(),
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),
        legend.position = "none") 

p8 <- brms::pp_check(m_timeSince_run1, ndraws = 100) + 
  scale_colour_manual(values = c("black", base_col[1]), name = "") +
  coord_cartesian(xlim = c(0, 500), expand = TRUE) + 
  theme_journal() + 
  theme(axis.line.y = element_blank(),
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),
        legend.position = "none") 

p9 <- brms::pp_check(m_timeSince_run2, ndraws = 100) + 
  scale_colour_manual(values = c("black", base_col[1]), name = "") +
  coord_cartesian(xlim = c(0, 2000), expand = TRUE) + 
  theme_journal() + 
  theme(axis.line.y = element_blank(),
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank(),
        legend.position = "none") 

# Solution to change the linewidth found here: https://stackoverflow.com/questions/55450441/how-to-change-size-from-specific-geom-in-ggplot2
p1$layers[[1]]$aes_params$linewidth <- 0.1
p1$layers[[2]]$aes_params$linewidth <- 0.1
p2$layers[[1]]$aes_params$linewidth <- 0.1
p2$layers[[2]]$aes_params$linewidth <- 0.1
p3$layers[[1]]$aes_params$linewidth <- 0.1
p3$layers[[2]]$aes_params$linewidth <- 0.1
p4$layers[[1]]$aes_params$linewidth <- 0.1
p4$layers[[2]]$aes_params$linewidth <- 0.1
p6$layers[[1]]$aes_params$linewidth <- 0.1
p6$layers[[2]]$aes_params$linewidth <- 0.1
p7$layers[[1]]$aes_params$linewidth <- 0.1
p7$layers[[2]]$aes_params$linewidth <- 0.1
p8$layers[[1]]$aes_params$linewidth <- 0.1
p8$layers[[2]]$aes_params$linewidth <- 0.1
p9$layers[[1]]$aes_params$linewidth <- 0.1
p9$layers[[2]]$aes_params$linewidth <- 0.1

# Combine plots
p_comb <- plot_grid(p1, p2, p3, p4, p6, p7, p8, p9, 
                    labels = "auto", label_size = 10, align = "hv")

ggsave(p_comb,
       filename = paste0(figurePath, "Fig_S11_ppc.png"), 
       dpi = dpi,
       width = 90,
       height = 90,
       units = "mm")
```

### Distribuion of starting relative to object locations
```{r distribution_of_starting_locations}
# Use one participant for visualisation because they are all the same anyway
start_vis_data <- OLM_7T_encoding[OLM_7T_encoding$subject == unique(OLM_7T_encoding$subject)[1], ]

# Remove the gift box that location was variable
no_gift <- obj_locations[obj_locations$objectName != "gift", ]

# Create the plot
start_locations <- ggplot() +
  # Circles
  geom_polygon(data = circle1, mapping = aes(x = x, y = z), fill = "lightgrey") +
  geom_polygon(data = circle2, mapping = aes(x = x, y = z), fill = "darkgrey") +
  geom_point(data = start_vis_data, mapping = aes(x = start_x, y = start_z, colour = targetNames)) +
  geom_segment(data = start_vis_data, mapping = aes(x = start_x, y = start_z, xend = object_x, yend = object_z, 
                                                    colour = targetNames)) +
  geom_point(data = no_gift, mapping = aes(x = object_x, y = object_z, colour = targetNames), size = 2) +
  scale_colour_viridis_d(option = "turbo") +
  # Other settings
  coord_equal(xlim = c(-90, 90)) +
  theme_void() +
  theme(plot.background = element_rect(fill = "white", colour = "white")) +
  labs(colour = "") +
  theme(legend.text = element_text(size=rel(0.5)),
        legend.key.height = unit(0.5,"line"),
        legend.key.width = unit(0.1,"line"),
        legend.title = element_blank(),
        legend.box.margin = margin(0, 0, 0, -10))

ggsave(start_locations,
       filename = paste0(figurePath, "Fig_S12_start_locations.png"), 
       dpi = dpi, width = 80, height = 80, units = "mm")
```

### Correlation between novelty measures
```{r corr_novelty_measures}
###############################################################################_
# Number of visits
# Set range
x1_range   <- range(m_visits_run1$data$s_onset_rel)
x1_points  <- seq_min_2_max(m_visits_run1$data$s_onset_rel)
x2_range   <- range(m_visits_run2$data$s_onset_rel) 
x2_points  <- seq_min_2_max(m_visits_run2$data$s_onset_rel)

# Get subject-level predictions
pred1_df <- predictions(m_visits_run1, newdata = datagrid(s_onset_rel = x1_points, subject = unique), 
                           by = c("s_onset_rel", "subject"))
pred2_df <- predictions(m_visits_run2, newdata = datagrid(s_onset_rel = x2_points, subject = unique), 
                           by = c("s_onset_rel", "subject"))
pred1_df <- as.data.frame(pred1_df)
pred2_df <- as.data.frame(pred2_df)

# Calculate the minimum for each subject, which is used for colouring
pred1_df <- ddply(pred1_df, c("subject"), mutate, min = min(estimate))
pred2_df <- ddply(pred2_df, c("subject"), mutate, min = min(estimate))

# Create plots
## Run 1
p1_1 <- ggplot(data = pred1_df, aes(x = s_onset_rel, y = estimate, group = subject, colour = min)) +
  geom_line() +
  scale_color_viridis_c() + theme_journal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 2, 1, 2), "mm")) +
  labs(title = "Run 1", x = "Time", y = "Number of visits") +
  scale_x_continuous(breaks = x1_range, labels = c("Start", "End")) +
  scale_y_continuous(breaks = c(2, 6, 10)) +
  coord_cartesian(xlim = x1_range, ylim = c(2, 10), expand = FALSE) 

## Run 2
p1_2 <- ggplot(data = pred2_df, aes(x = s_onset_rel, y = estimate, group = subject, colour = min)) +
  geom_line() +
  scale_color_viridis_c() + theme_journal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 2, 1, 2), "mm")) +
  labs(title = "Run 2", x = "Time", y = "Number of visits") +
  scale_x_continuous(breaks = x2_range, labels = c("Start", "End")) +
  scale_y_continuous(breaks = c(6, 10, 14)) +
  coord_cartesian(xlim = x2_range, ylim = c(6, 14), expand = FALSE) 

###############################################################################_
# Get original scale for time since
# Load 
load("event_tables/images/SpaNov_event_file_gradients.RData")

# Convert list to data frame
data <- rbindlist(subj_list, idcol = "subject")

# Add subjects' R number
data$ppid <- subjIDs_R[data$subject]

# Function to match runStartTime
find_runStartTime <- function(ppid, run){
  # Get corresponding to find the run in the trial data
  anonKey <- lookupTable$anonKey[lookupTable$Rnum == ppid[1]]
  
  # Use the anonKey & run to get runStartTime
  runStartTime <- OLM_7T_trial_results$runStartTime[OLM_7T_trial_results$subject == anonKey & 
                                                    OLM_7T_trial_results$block_num == run[1]]
  
  return(runStartTime[1])
}

data <- ddply(data, c("subject", "ppid", "run"), mutate, 
              runStartTime = find_runStartTime(ppid, run))

data$subject <- as.character(data$subject)

# Make time relative to the start of the run. The real onset times will be 
# slightly different but this will not matter for this. 
data$onset_rel <- data$onset - data$runStartTime

# Add run type
data$runType <- "encoding"
data$runType[data$run == 2 | data$run == 4] <- "retrieval"

# Subset to only encoding
data_sub <- data[data$runType == "encoding", ]

# Change run number to match the description in paper. Run 3 is Encoding Run 2
data_sub$run[data_sub$run == 3] <- 2

# Convert run to factor
data_sub$f_run <- as.factor(data_sub$run)

# Run 1 
## Remove NA and get correct run
## Use unique name for data frame because of marginaleffects issues
m_timeSince_run1_data <- na.omit(data_sub[data_sub$run == 1, ])

## Scale x and y
m_timeSince_run1_data$s_onset_rel  <- scale_m0_sd0.5(m_timeSince_run1_data$onset_rel)
m_timeSince_run1_data$s_timeSince  <- drop(scale(m_timeSince_run1_data$lastVisit)) 
# Use drop() to deal with marginal effects issues

# Run 2 
## Remove NA and get correct run
## Use unique name for data frame because of marginaleffects issues
m_timeSince_run2_data <- na.omit(data_sub[data_sub$run == 2, ])

## Scale x and y
m_timeSince_run2_data$s_onset_rel  <- scale_m0_sd0.5(m_timeSince_run2_data$onset_rel)
m_timeSince_run2_data$s_timeSince  <- drop(scale(m_timeSince_run2_data$lastVisit))
# Use drop() to deal with marginal effects issues

###############################################################################_
# Time since
# Set range
x1_range   <- range(m_timeSince_run1$data$s_onset_rel)
x1_points  <- seq_min_2_max(m_timeSince_run1$data$s_onset_rel)
x2_range   <- range(m_timeSince_run2$data$s_onset_rel) 
x2_points  <- seq_min_2_max(m_timeSince_run2$data$s_onset_rel)

# Get subject-level predictions
pred1_df <- predictions(m_timeSince_run1, newdata = datagrid(s_onset_rel = x1_points, subject = unique), 
                           by = c("s_onset_rel", "subject"))
pred2_df <- predictions(m_timeSince_run2, newdata = datagrid(s_onset_rel = x2_points, subject = unique), 
                           by = c("s_onset_rel", "subject"))
pred1_df <- as.data.frame(pred1_df)
pred2_df <- as.data.frame(pred2_df)

# Calculate the minimum for each subject, which is used for colouring
pred1_df <- ddply(pred1_df, c("subject"), mutate, min = min(estimate))
pred2_df <- ddply(pred2_df, c("subject"), mutate, min = min(estimate))

# Create plots
## Run 1
p2_1 <- ggplot(data = pred1_df, aes(x = s_onset_rel, y = estimate, group = subject, colour = min)) +
  geom_line() +
  scale_color_viridis_c() + theme_journal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 2, 1, 2), "mm")) +
  labs(title = "Run 1", x = "Time", y = "Time elapsed (seconds)") +
  scale_x_continuous(breaks = x1_range, labels = c("Start", "End")) +
  scale_y_continuous(breaks = c(50, 125, 200)) +
  coord_cartesian(xlim = x1_range, ylim = c(50, 200), expand = FALSE) 

## Run 2
p2_2 <- ggplot(data = pred2_df, aes(x = s_onset_rel, y = estimate, group = subject, colour = min)) +
  geom_line() +
  scale_color_viridis_c() + theme_journal() +
  theme(legend.position = "none",
        plot.margin = unit(c(1, 2, 1, 2), "mm")) +
  labs(title = "Run 2", x = "Time", y = "Time elapsed (seconds)") +
  scale_x_continuous(breaks = x2_range, labels = c("Start", "End")) +
  scale_y_continuous(breaks = c(50, 300, 550)) +
  coord_cartesian(xlim = x2_range, ylim = c(50, 550), expand = FALSE) 

###############################################################################_
# Calculate correlation
temp_corr <- cor.test(event_info_df_encoding$visits, event_info_df_encoding$lastVisit)
plot_str  <- paste("r", rValue(temp_corr$estimate), "p", pValue(temp_corr$p.value))

# Create scatter plot
corr_novelty_measures <- ggplot(event_info_df_encoding, aes(x = visits, y = lastVisit)) +
  geom_point(alpha = 0.1) + 
  geom_smooth(method = "lm", formula = "y ~ x", colour = base_col[1]) +
  annotate("text", x = I(0.8), y = I(0.8), label = plot_str, size = 2) +
  theme_journal() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = 0.5)) +
  labs(x = "Number of visits", y = "Time elapsed (seconds)",
       title = "Relationship between novelty measures")

###############################################################################_
# Combine into one figure
combined_plot <- plot_grid(corr_novelty_measures, plot_grid(p1_1, p1_2, p2_1, p2_2, 
                                                            ncol = 2, labels = c("b", "", "c", ""), 
                                                            align = "hv",  label_size = 10),
                           ncol = 2, labels = c("a", ""), label_size = 10)

# Save
ggsave(combined_plot,
       filename = paste0(figurePath, "Fig_S13_corr_novelty_measures.png"), 
       dpi = dpi, width = 190, height = 82, units = "mm")
```